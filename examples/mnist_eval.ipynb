{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation with AdaPT on MNIST dataset\n",
    "\n",
    "In this notebook you can evaluate different approximate multipliers on various models based on MNIST dataset\n",
    "\n",
    "Steps:\n",
    "* Select number of threads to use\n",
    "* Load dataset\n",
    "* Load Adapt Layers\n",
    "* Define Model\n",
    "* Run model calibration for quantization\n",
    "* Evaluate\n",
    "\n",
    "\n",
    "**Note**:\n",
    "* This notebook should be run on a X86 machine\n",
    "\n",
    "* Please make sure you have run the installation steps first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select number of threads to use\n",
    "\n",
    "For optimal performance set them as the number of your cpu threads (not cpu cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_PLACES=cores\n",
      "env: OMP_PROC_BIND=close\n",
      "env: OMP_WAIT_POLICY=active\n"
     ]
    }
   ],
   "source": [
    "threads = 40\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "#maybe better performance\n",
    "%env OMP_PLACES=cores\n",
    "%env OMP_PROC_BIND=close\n",
    "%env OMP_WAIT_POLICY=active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST('./datasets/mnist_data/data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "test = MNIST('./datasets/mnist_data/data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # ToTensor does min-max normalization. \n",
    "]), )\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=64,num_workers=1, pin_memory=False)\n",
    "train_loader = dataloader.DataLoader(train, **dataloader_args)\n",
    "test_loader = dataloader.DataLoader(test, **dataloader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Adapt Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load ADAPT layers\n",
    "from adapt.approx_layers import axx_layers as approxNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose approximate multiplier \n",
    "\n",
    "Two approximate multipliers are already provided\n",
    "\n",
    "**mul8s_acc** - (header file: mul8s_acc.h)   <--  default\n",
    "\n",
    "**mul8s_1L2H** - (header file: mul8s_1L2H.h)\n",
    "\n",
    "\n",
    "\n",
    "In order to use your custom multiplier you need to use the provided tool (LUT_generator) to easily create the C++ header for your multiplier. Then you just place it inside the adapt/cpu-kernels/axx_mults folder. The name of the axx_mult here must match the name of the header file. The same axx_mult is used in all layers. \n",
    "\n",
    "Tip: If you want explicitly to set for each layer a different axx_mult you must do it from the model definition using the respective AdaPT_Conv2d class of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "axx_mult = 'mul8s_1L2H'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "Jit compilation method loads 'on the fly' the C++ extentions of the approximate multipliers. Then the pytorch model is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_linear_mul8s_1L2H/build.ninja...\n",
      "Building extension module PyInit_linear_mul8s_1L2H...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF axx_linear.o.d -DTORCH_EXTENSION_NAME=PyInit_linear_mul8s_1L2H -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.8/dist-packages/torch/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.8/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.8/dist-packages/torch/include/THC -isystem /usr/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -DAXX_MULT=mul8s_1L2H -march=native -fopenmp -O3 -c /workspace/adapt/adapt/cpu-kernels/axx_linear.cpp -o axx_linear.o \n",
      "[2/2] c++ axx_linear.o -shared -lgomp -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o PyInit_linear_mul8s_1L2H.so\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_1L2H, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_1L2H, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_1L2H...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set flag for use of AdaPT custom layers or vanilla PyTorch\n",
    "use_adapt=True\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        if use_adapt:\n",
    "             self.fc1 = approxNN.AdaPT_Linear(784, 548, axx_mult = axx_mult)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(784, 548)\n",
    "        \n",
    "        self.bc1 = nn.BatchNorm1d(548)\n",
    "\n",
    "        if use_adapt:\n",
    "            self.fc2 = approxNN.AdaPT_Linear(548, 252, axx_mult = axx_mult)\n",
    "        else:    \n",
    "            self.fc2 = nn.Linear(548, 252)\n",
    "            \n",
    "        self.bc2 = nn.BatchNorm1d(252)     \n",
    "        \n",
    "        if use_adapt:\n",
    "            self.fc3 = approxNN.AdaPT_Linear(252, 10, axx_mult = axx_mult)\n",
    "        else:\n",
    "            self.fc3 = nn.Linear(252, 10)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 784))\n",
    "        h = self.fc1(x)\n",
    "        h = self.bc1(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.5, training=self.training)\n",
    "        \n",
    "        h = self.fc2(h)\n",
    "        h = self.bc2(h)\n",
    "        h = F.relu(h)\n",
    "        h = F.dropout(h, p=0.2, training=self.training)\n",
    "        \n",
    "        h = self.fc3(h)\n",
    "        out = F.log_softmax(h,-1)\n",
    "        return out\n",
    "\n",
    "model = Model()\n",
    "model.cpu()\n",
    "\n",
    "#load pretrained weights\n",
    "model.load_state_dict(torch.load('models/state_dicts/mnist.pt'))\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters \n",
    "\n",
    "Need to re-run it each time the model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0918 15:56:36.710333 140050416883520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0918 15:56:36.710931 140050416883520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0918 15:56:36.711207 140050416883520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0918 15:56:36.711464 140050416883520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0918 15:56:36.711718 140050416883520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0918 15:56:36.711956 140050416883520 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0918 15:56:36.714071 140050416883520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0918 15:56:36.714332 140050416883520 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0918 15:56:36.714918 140050416883520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0918 15:56:36.715378 140050416883520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0918 15:56:36.715716 140050416883520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0918 15:56:36.716084 140050416883520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0918 15:56:36.716431 140050416883520 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.quantizer                           : TensorQuantizer(8bit per-tensor amax=254.8755 calibrator=HistogramCalibrator quant)\n",
      "fc1.quantizer_w                         : TensorQuantizer(8bit per-tensor amax=0.5369 calibrator=HistogramCalibrator quant)\n",
      "fc2.quantizer                           : TensorQuantizer(8bit per-tensor amax=5.7996 calibrator=HistogramCalibrator quant)\n",
      "fc2.quantizer_w                         : TensorQuantizer(8bit per-tensor amax=0.4709 calibrator=HistogramCalibrator quant)\n",
      "fc3.quantizer                           : TensorQuantizer(8bit per-tensor amax=6.2061 calibrator=HistogramCalibrator quant)\n",
      "fc3.quantizer_w                         : TensorQuantizer(8bit per-tensor amax=0.5777 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "        \n",
    "     evaluate_x = Variable(data_loader.dataset.data.type_as(torch.FloatTensor())).cpu()\n",
    "     model(evaluate_x)\n",
    "        \n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, test_loader, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: tensor(0.9754)\n"
     ]
    }
   ],
   "source": [
    "evaluate_x = Variable(test_loader.dataset.data.type_as(torch.FloatTensor())).cpu()\n",
    "evaluate_y = Variable(test_loader.dataset.targets).cpu()\n",
    "\n",
    "\n",
    "output = model(evaluate_x)\n",
    "pred = output.data.max(1)[1]\n",
    "d = pred.eq(evaluate_y.data).cpu()\n",
    "accuracy = d.sum()/d.size()[0]\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
