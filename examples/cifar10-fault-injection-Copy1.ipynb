{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a115dd",
   "metadata": {},
   "source": [
    "# Model evaluation and re-training with AdaPT on Cifar10 dataset\n",
    "\n",
    "In this notebook you can evaluate different approximate multipliers on various models based on Cifar10 dataset\n",
    "\n",
    "Steps:\n",
    "* Select models to load \n",
    "* Select number of threads to use\n",
    "* Choose approximate multiplier \n",
    "* Load model for evaluation\n",
    "* Load dataset\n",
    "* Run model calibration for quantization\n",
    "* Run model evaluation\n",
    "* Run approximate-aware re-training\n",
    "* Rerun model evaluation\n",
    "\n",
    "**Note**:\n",
    "* This notebook should be run on a X86 machine\n",
    "\n",
    "* Please make sure you have run the installation steps first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5eef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d4c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_seed():\n",
    "    return 1221 # 121 and 1221\n",
    "\n",
    "def set_random_seeds():\n",
    "    torch.manual_seed(get_random_seed())\n",
    "    np.random.seed(get_random_seed())\n",
    "    random.seed(get_random_seed())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab49b35",
   "metadata": {},
   "source": [
    "## Select models to load \n",
    "\n",
    "The weights must be downloaded in state_dicts folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183a13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.SDNs.vgg_sdn import vgg16_sdn_bn\n",
    "# from models.SDNs.approximate_hardware_multiplier import wideresnet_sdn_v1\n",
    "from models.SDNs.wideresnet_sdn import wideresnet_sdn_v1\n",
    "import models.SDNs.fault_injection as fie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69265983",
   "metadata": {},
   "source": [
    "## Select number of threads to use\n",
    "\n",
    "For optimal performance set them as the number of your cpu threads (not cpu cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165c2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of CPU cores: 64\n",
      "env: OMP_PLACES=cores\n",
      "env: OMP_PROC_BIND=close\n",
      "env: OMP_WAIT_POLICY=active\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get the total number of CPU cores\n",
    "total_cores = psutil.cpu_count()\n",
    "print(f\"Total number of CPU cores: {total_cores}\")\n",
    "\n",
    "threads = int(total_cores / 2)\n",
    "threads = 46\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "# maybe better performance\n",
    "%env OMP_PLACES=cores\n",
    "%env OMP_PROC_BIND=close\n",
    "%env OMP_WAIT_POLICY=active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa06300",
   "metadata": {},
   "source": [
    "## Choose approximate multiplier \n",
    "\n",
    "Two approximate multipliers are already provided\n",
    "\n",
    "**mul8s_acc** - (header file: mul8s_acc.h)   <--  default\n",
    "\n",
    "**mul8s_1L2H** - (header file: mul8s_1L2H.h)\n",
    "\n",
    "\n",
    "\n",
    "In order to use your custom multiplier you need to use the provided tool (LUT_generator) to easily create the C++ header for your multiplier. Then you just place it inside the adapt/cpu-kernels/axx_mults folder. The name of the axx_mult here must match the name of the header file. The same axx_mult is used in all layers. \n",
    "\n",
    "Tip: If you want explicitly to set for each layer a different axx_mult you must do it from the model definition using the respective AdaPT_Conv2d class of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562689c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "axx_mult = 'mul8s_acc'\n",
    "# axx_mult = 'mul8s_1L2H'\n",
    "\n",
    "# axx_mult = 'mul8s_1L2L'\n",
    "\n",
    "# axx_mult = 'mul8s_1L2N'\n",
    "# axx_mult = 'mul8s_1L12'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539e7e1",
   "metadata": {},
   "source": [
    "## Load model for evaluation\n",
    "\n",
    "Jit compilation method loads 'on the fly' the C++ extentions of the approximate multipliers. Then the pytorch model is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc26796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /home/kkfdh/.cache/torch_extensions/PyInit_conv2d_mul8s_acc/build.ninja...\n",
      "Building extension module PyInit_conv2d_mul8s_acc...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /home/kkfdh/.cache/torch_extensions/PyInit_linear_mul8s_acc/build.ninja...\n",
      "Building extension module PyInit_linear_mul8s_acc...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /home/kkfdh/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WideResNet_SDN(\n",
       "  (init_conv): AdaPT_Conv2d(\n",
       "    3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): AdaPT_Conv2d(\n",
       "            16, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (2): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (4): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): AdaPT_Conv2d(\n",
       "            64, 128, kernel_size=(1, 1), stride=(2, 2)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (8): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (10): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): AdaPT_Conv2d(\n",
       "            128, 256, kernel_size=(1, 1), stride=(2, 2)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (12): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (14): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (end_layers): Sequential(\n",
       "    (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): AdaPT_Linear(\n",
       "      (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = vgg16_sdn_bn(pretrained=True, axx_mult = axx_mult)\n",
    "model = wideresnet_sdn_v1(pretrained=True, axx_mult = axx_mult)\n",
    "\n",
    "model.eval() # for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de58a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "init_conv\n",
      "init_conv.quantizer\n",
      "init_conv.quantizer_w\n",
      "layers\n",
      "layers.0\n",
      "layers.0.layers\n",
      "layers.0.layers.0\n",
      "layers.0.layers.0.0\n",
      "layers.0.layers.0.1\n",
      "layers.0.layers.0.2\n",
      "layers.0.layers.0.2.quantizer\n",
      "layers.0.layers.0.2.quantizer_w\n",
      "layers.0.layers.0.3\n",
      "layers.0.layers.0.4\n",
      "layers.0.layers.0.5\n",
      "layers.0.layers.0.6\n",
      "layers.0.layers.0.6.quantizer\n",
      "layers.0.layers.0.6.quantizer_w\n",
      "layers.0.layers.1\n",
      "layers.0.layers.1.0\n",
      "layers.0.layers.1.0.quantizer\n",
      "layers.0.layers.1.0.quantizer_w\n",
      "layers.1\n",
      "layers.1.layers\n",
      "layers.1.layers.0\n",
      "layers.1.layers.0.0\n",
      "layers.1.layers.0.1\n",
      "layers.1.layers.0.2\n",
      "layers.1.layers.0.2.quantizer\n",
      "layers.1.layers.0.2.quantizer_w\n",
      "layers.1.layers.0.3\n",
      "layers.1.layers.0.4\n",
      "layers.1.layers.0.5\n",
      "layers.1.layers.0.6\n",
      "layers.1.layers.0.6.quantizer\n",
      "layers.1.layers.0.6.quantizer_w\n",
      "layers.1.layers.1\n",
      "layers.2\n",
      "layers.2.layers\n",
      "layers.2.layers.0\n",
      "layers.2.layers.0.0\n",
      "layers.2.layers.0.1\n",
      "layers.2.layers.0.2\n",
      "layers.2.layers.0.2.quantizer\n",
      "layers.2.layers.0.2.quantizer_w\n",
      "layers.2.layers.0.3\n",
      "layers.2.layers.0.4\n",
      "layers.2.layers.0.5\n",
      "layers.2.layers.0.6\n",
      "layers.2.layers.0.6.quantizer\n",
      "layers.2.layers.0.6.quantizer_w\n",
      "layers.2.layers.1\n",
      "layers.2.output\n",
      "layers.2.output.max_pool\n",
      "layers.2.output.avg_pool\n",
      "layers.2.output.linear\n",
      "layers.2.output.linear.quantizer\n",
      "layers.2.output.linear.quantizer_w\n",
      "layers.3\n",
      "layers.3.layers\n",
      "layers.3.layers.0\n",
      "layers.3.layers.0.0\n",
      "layers.3.layers.0.1\n",
      "layers.3.layers.0.2\n",
      "layers.3.layers.0.2.quantizer\n",
      "layers.3.layers.0.2.quantizer_w\n",
      "layers.3.layers.0.3\n",
      "layers.3.layers.0.4\n",
      "layers.3.layers.0.5\n",
      "layers.3.layers.0.6\n",
      "layers.3.layers.0.6.quantizer\n",
      "layers.3.layers.0.6.quantizer_w\n",
      "layers.3.layers.1\n",
      "layers.4\n",
      "layers.4.layers\n",
      "layers.4.layers.0\n",
      "layers.4.layers.0.0\n",
      "layers.4.layers.0.1\n",
      "layers.4.layers.0.2\n",
      "layers.4.layers.0.2.quantizer\n",
      "layers.4.layers.0.2.quantizer_w\n",
      "layers.4.layers.0.3\n",
      "layers.4.layers.0.4\n",
      "layers.4.layers.0.5\n",
      "layers.4.layers.0.6\n",
      "layers.4.layers.0.6.quantizer\n",
      "layers.4.layers.0.6.quantizer_w\n",
      "layers.4.layers.1\n",
      "layers.4.output\n",
      "layers.4.output.max_pool\n",
      "layers.4.output.avg_pool\n",
      "layers.4.output.linear\n",
      "layers.4.output.linear.quantizer\n",
      "layers.4.output.linear.quantizer_w\n",
      "layers.5\n",
      "layers.5.layers\n",
      "layers.5.layers.0\n",
      "layers.5.layers.0.0\n",
      "layers.5.layers.0.1\n",
      "layers.5.layers.0.2\n",
      "layers.5.layers.0.2.quantizer\n",
      "layers.5.layers.0.2.quantizer_w\n",
      "layers.5.layers.0.3\n",
      "layers.5.layers.0.4\n",
      "layers.5.layers.0.5\n",
      "layers.5.layers.0.6\n",
      "layers.5.layers.0.6.quantizer\n",
      "layers.5.layers.0.6.quantizer_w\n",
      "layers.5.layers.1\n",
      "layers.5.layers.1.0\n",
      "layers.5.layers.1.0.quantizer\n",
      "layers.5.layers.1.0.quantizer_w\n",
      "layers.6\n",
      "layers.6.layers\n",
      "layers.6.layers.0\n",
      "layers.6.layers.0.0\n",
      "layers.6.layers.0.1\n",
      "layers.6.layers.0.2\n",
      "layers.6.layers.0.2.quantizer\n",
      "layers.6.layers.0.2.quantizer_w\n",
      "layers.6.layers.0.3\n",
      "layers.6.layers.0.4\n",
      "layers.6.layers.0.5\n",
      "layers.6.layers.0.6\n",
      "layers.6.layers.0.6.quantizer\n",
      "layers.6.layers.0.6.quantizer_w\n",
      "layers.6.layers.1\n",
      "layers.6.output\n",
      "layers.6.output.max_pool\n",
      "layers.6.output.avg_pool\n",
      "layers.6.output.linear\n",
      "layers.6.output.linear.quantizer\n",
      "layers.6.output.linear.quantizer_w\n",
      "layers.7\n",
      "layers.7.layers\n",
      "layers.7.layers.0\n",
      "layers.7.layers.0.0\n",
      "layers.7.layers.0.1\n",
      "layers.7.layers.0.2\n",
      "layers.7.layers.0.2.quantizer\n",
      "layers.7.layers.0.2.quantizer_w\n",
      "layers.7.layers.0.3\n",
      "layers.7.layers.0.4\n",
      "layers.7.layers.0.5\n",
      "layers.7.layers.0.6\n",
      "layers.7.layers.0.6.quantizer\n",
      "layers.7.layers.0.6.quantizer_w\n",
      "layers.7.layers.1\n",
      "layers.8\n",
      "layers.8.layers\n",
      "layers.8.layers.0\n",
      "layers.8.layers.0.0\n",
      "layers.8.layers.0.1\n",
      "layers.8.layers.0.2\n",
      "layers.8.layers.0.2.quantizer\n",
      "layers.8.layers.0.2.quantizer_w\n",
      "layers.8.layers.0.3\n",
      "layers.8.layers.0.4\n",
      "layers.8.layers.0.5\n",
      "layers.8.layers.0.6\n",
      "layers.8.layers.0.6.quantizer\n",
      "layers.8.layers.0.6.quantizer_w\n",
      "layers.8.layers.1\n",
      "layers.8.output\n",
      "layers.8.output.max_pool\n",
      "layers.8.output.avg_pool\n",
      "layers.8.output.linear\n",
      "layers.8.output.linear.quantizer\n",
      "layers.8.output.linear.quantizer_w\n",
      "layers.9\n",
      "layers.9.layers\n",
      "layers.9.layers.0\n",
      "layers.9.layers.0.0\n",
      "layers.9.layers.0.1\n",
      "layers.9.layers.0.2\n",
      "layers.9.layers.0.2.quantizer\n",
      "layers.9.layers.0.2.quantizer_w\n",
      "layers.9.layers.0.3\n",
      "layers.9.layers.0.4\n",
      "layers.9.layers.0.5\n",
      "layers.9.layers.0.6\n",
      "layers.9.layers.0.6.quantizer\n",
      "layers.9.layers.0.6.quantizer_w\n",
      "layers.9.layers.1\n",
      "layers.10\n",
      "layers.10.layers\n",
      "layers.10.layers.0\n",
      "layers.10.layers.0.0\n",
      "layers.10.layers.0.1\n",
      "layers.10.layers.0.2\n",
      "layers.10.layers.0.2.quantizer\n",
      "layers.10.layers.0.2.quantizer_w\n",
      "layers.10.layers.0.3\n",
      "layers.10.layers.0.4\n",
      "layers.10.layers.0.5\n",
      "layers.10.layers.0.6\n",
      "layers.10.layers.0.6.quantizer\n",
      "layers.10.layers.0.6.quantizer_w\n",
      "layers.10.layers.1\n",
      "layers.10.layers.1.0\n",
      "layers.10.layers.1.0.quantizer\n",
      "layers.10.layers.1.0.quantizer_w\n",
      "layers.10.output\n",
      "layers.10.output.max_pool\n",
      "layers.10.output.avg_pool\n",
      "layers.10.output.linear\n",
      "layers.10.output.linear.quantizer\n",
      "layers.10.output.linear.quantizer_w\n",
      "layers.11\n",
      "layers.11.layers\n",
      "layers.11.layers.0\n",
      "layers.11.layers.0.0\n",
      "layers.11.layers.0.1\n",
      "layers.11.layers.0.2\n",
      "layers.11.layers.0.2.quantizer\n",
      "layers.11.layers.0.2.quantizer_w\n",
      "layers.11.layers.0.3\n",
      "layers.11.layers.0.4\n",
      "layers.11.layers.0.5\n",
      "layers.11.layers.0.6\n",
      "layers.11.layers.0.6.quantizer\n",
      "layers.11.layers.0.6.quantizer_w\n",
      "layers.11.layers.1\n",
      "layers.12\n",
      "layers.12.layers\n",
      "layers.12.layers.0\n",
      "layers.12.layers.0.0\n",
      "layers.12.layers.0.1\n",
      "layers.12.layers.0.2\n",
      "layers.12.layers.0.2.quantizer\n",
      "layers.12.layers.0.2.quantizer_w\n",
      "layers.12.layers.0.3\n",
      "layers.12.layers.0.4\n",
      "layers.12.layers.0.5\n",
      "layers.12.layers.0.6\n",
      "layers.12.layers.0.6.quantizer\n",
      "layers.12.layers.0.6.quantizer_w\n",
      "layers.12.layers.1\n",
      "layers.12.output\n",
      "layers.12.output.max_pool\n",
      "layers.12.output.avg_pool\n",
      "layers.12.output.linear\n",
      "layers.12.output.linear.quantizer\n",
      "layers.12.output.linear.quantizer_w\n",
      "layers.13\n",
      "layers.13.layers\n",
      "layers.13.layers.0\n",
      "layers.13.layers.0.0\n",
      "layers.13.layers.0.1\n",
      "layers.13.layers.0.2\n",
      "layers.13.layers.0.2.quantizer\n",
      "layers.13.layers.0.2.quantizer_w\n",
      "layers.13.layers.0.3\n",
      "layers.13.layers.0.4\n",
      "layers.13.layers.0.5\n",
      "layers.13.layers.0.6\n",
      "layers.13.layers.0.6.quantizer\n",
      "layers.13.layers.0.6.quantizer_w\n",
      "layers.13.layers.1\n",
      "layers.14\n",
      "layers.14.layers\n",
      "layers.14.layers.0\n",
      "layers.14.layers.0.0\n",
      "layers.14.layers.0.1\n",
      "layers.14.layers.0.2\n",
      "layers.14.layers.0.2.quantizer\n",
      "layers.14.layers.0.2.quantizer_w\n",
      "layers.14.layers.0.3\n",
      "layers.14.layers.0.4\n",
      "layers.14.layers.0.5\n",
      "layers.14.layers.0.6\n",
      "layers.14.layers.0.6.quantizer\n",
      "layers.14.layers.0.6.quantizer_w\n",
      "layers.14.layers.1\n",
      "end_layers\n",
      "end_layers.0\n",
      "end_layers.1\n",
      "end_layers.2\n",
      "end_layers.3\n",
      "end_layers.4\n",
      "end_layers.4.quantizer\n",
      "end_layers.4.quantizer_w\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "set_random_seeds()\n",
    "\n",
    "# Print names of immediate layers only\n",
    "for name, layer in model.named_modules():\n",
    "    print(name)\n",
    "    \n",
    "# layers.0.layers.0.quantizer\n",
    "# layers.0.layers.0.quantizer_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76721ed0",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63b4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128,\n",
    "                                            shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47aa7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTrigger(object):\n",
    "    def __init__(self, square_size=5, square_loc=(26,26)):\n",
    "        self.square_size = square_size\n",
    "        self.square_loc = square_loc\n",
    "\n",
    "    def __call__(self, pil_data):\n",
    "        square = Image.new('L', (self.square_size, self.square_size), 255)\n",
    "        pil_data.paste(square, self.square_loc)\n",
    "        return pil_data\n",
    "\n",
    "class Cifar10_:\n",
    "    def __init__(self, batch_size=128, add_trigger=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = 32\n",
    "        self.num_classes = 10\n",
    "        self.num_test = 10000\n",
    "        self.num_train = 50000\n",
    "\n",
    "        normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.augmented = T.Compose([T.RandomHorizontalFlip(), T.RandomCrop(32, padding=4),T.ToTensor(), normalize])\n",
    "\n",
    "        self.normalized = T.Compose([T.ToTensor(), normalize])\n",
    "\n",
    "        self.aug_trainset =  CIFAR10(root='datasets/cifar10_data', train=True, download=False, transform=self.augmented)\n",
    "        self.aug_train_loader = torch.utils.data.DataLoader(self.aug_trainset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        self.trainset =  CIFAR10(root='datasets/cifar10_data', train=True, download=False, transform=self.normalized)\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.testset =  CIFAR10(root='datasets/cifar10_data', train=False, download=False, transform=self.normalized)\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        # add trigger to the test set samples\n",
    "        # for the experiments on the backdoored CNNs and SDNs\n",
    "        #  uncomment third line to measure backdoor attack success, right now it measures standard accuracy\n",
    "        if add_trigger: \n",
    "            self.trigger_transform = T.Compose([AddTrigger(), T.ToTensor(), normalize])\n",
    "            self.trigger_test_set = CIFAR10(root='datasets/cifar10_data', train=False, download=False, transform=self.trigger_transform)\n",
    "            # self.trigger_test_set.test_labels = [5] * self.num_test\n",
    "            self.trigger_test_loader = torch.utils.data.DataLoader(self.trigger_test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "def load_cifar10(batch_size, add_trigger=False):\n",
    "    cifar10_data = Cifar10_(batch_size=batch_size, add_trigger=add_trigger)\n",
    "    return cifar10_data\n",
    "\n",
    "def get_dataset(batch_size=128, add_trigger=False):\n",
    "    return load_cifar10(batch_size, add_trigger)\n",
    "\n",
    "t_dataset = get_dataset()\n",
    "one_batch_dataset = get_dataset(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa74c5d",
   "metadata": {},
   "source": [
    "## Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters \n",
    "\n",
    "Need to re-run it each time the model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "946f0d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:21<00:00, 10.85s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0905 15:27:33.022399 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.023094 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.023473 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.023799 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.024515 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.024856 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.025215 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.025552 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.025890 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.026231 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.026556 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.027301 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.027650 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.027964 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.028493 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.028954 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.029253 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.029577 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.030105 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.030459 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.030776 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.031095 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.031451 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.031767 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.032505 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.032810 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.033157 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.033510 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.034093 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.034438 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.034761 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.035290 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.035703 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.036015 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.036383 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.036733 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.037058 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.037398 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.038103 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.038408 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.038890 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.039229 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.039535 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.039853 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.040526 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.040818 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.041141 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.041470 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.041784 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.042334 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.042808 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.043120 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.043464 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.043768 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.044092 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.044429 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.044750 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.045471 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.045789 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.046101 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.046443 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.046749 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.047085 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.050677 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.051021 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.051324 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.052178 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.052510 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.052820 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.053347 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.053660 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.053961 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.054285 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.054600 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.054909 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.055223 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.055550 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.055867 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.056179 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.056497 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.056813 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.057118 140499336595264 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 15:27:33.058400 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.058731 140499336595264 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0905 15:27:33.059476 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.060088 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.060678 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0905 15:27:33.061273 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.061899 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.062511 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.063149 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.063735 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.064327 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.064902 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.065527 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.066149 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.066734 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.067324 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.067901 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.068529 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.069108 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.069722 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.070327 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.070917 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.071487 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.072077 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.072654 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.073237 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.073809 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.074387 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.074995 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.075580 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.076184 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.076767 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.077347 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.077964 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.078536 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.079160 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.079745 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.080322 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.080920 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.081507 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.082110 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.082683 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.083251 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.083852 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.084443 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.085018 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.085581 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.086189 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.086750 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.087369 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.087931 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.088535 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.089101 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.089704 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.090315 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.090948 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.091554 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.092126 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.092729 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.093308 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.093914 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.094522 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.095116 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.095750 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.096326 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.096951 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.097422 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.098113 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.098717 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.099365 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.099999 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.100604 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.101179 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.101784 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.102407 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.103028 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.103635 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.104238 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.104830 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.105456 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.106040 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 15:27:33.106805 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0905 15:27:33.107400 140499336595264 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_conv.quantizer                     : TensorQuantizer(8bit per-tensor amax=2.6387 calibrator=HistogramCalibrator quant)\n",
      "init_conv.quantizer_w                   : TensorQuantizer(8bit per-tensor amax=0.8841 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=1.6063 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.2432 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=1.0522 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1076 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.1.0.quantizer           : TensorQuantizer(8bit per-tensor amax=3.4750 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.1.0.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.3361 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=1.3873 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1227 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.7407 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1022 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.9657 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1110 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.8380 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1184 calibrator=HistogramCalibrator quant)\n",
      "layers.2.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=3.9902 calibrator=HistogramCalibrator quant)\n",
      "layers.2.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.6924 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.8677 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1047 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.8880 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1019 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.9183 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1135 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=1.0576 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0889 calibrator=HistogramCalibrator quant)\n",
      "layers.4.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=2.4665 calibrator=HistogramCalibrator quant)\n",
      "layers.4.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.9195 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=1.5600 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1177 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.5081 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0894 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.1.0.quantizer           : TensorQuantizer(8bit per-tensor amax=1.5406 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.1.0.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.2459 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.8551 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0910 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.5062 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0724 calibrator=HistogramCalibrator quant)\n",
      "layers.6.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=2.4581 calibrator=HistogramCalibrator quant)\n",
      "layers.6.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.6053 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.7881 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0819 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.4489 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0680 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.7110 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0655 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.4795 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0610 calibrator=HistogramCalibrator quant)\n",
      "layers.8.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=2.2360 calibrator=HistogramCalibrator quant)\n",
      "layers.8.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.7146 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.6953 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0682 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.5032 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0606 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.2.quantizer          : TensorQuantizer(8bit per-tensor amax=1.2361 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.2.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0768 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.6.quantizer          : TensorQuantizer(8bit per-tensor amax=0.6440 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.6.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0552 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.1.0.quantizer          : TensorQuantizer(8bit per-tensor amax=0.9508 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.1.0.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.1231 calibrator=HistogramCalibrator quant)\n",
      "layers.10.output.linear.quantizer       : TensorQuantizer(8bit per-tensor amax=1.9157 calibrator=HistogramCalibrator quant)\n",
      "layers.10.output.linear.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.5368 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.2.quantizer          : TensorQuantizer(8bit per-tensor amax=0.7588 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.2.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0487 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.6.quantizer          : TensorQuantizer(8bit per-tensor amax=0.5279 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.6.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0475 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.2.quantizer          : TensorQuantizer(8bit per-tensor amax=0.7213 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.2.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0547 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.6.quantizer          : TensorQuantizer(8bit per-tensor amax=0.6774 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.6.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0447 calibrator=HistogramCalibrator quant)\n",
      "layers.12.output.linear.quantizer       : TensorQuantizer(8bit per-tensor amax=2.3778 calibrator=HistogramCalibrator quant)\n",
      "layers.12.output.linear.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.4971 calibrator=HistogramCalibrator quant)\n",
      "layers.13.layers.0.2.quantizer          : TensorQuantizer(8bit per-tensor amax=0.8459 calibrator=HistogramCalibrator quant)\n",
      "layers.13.layers.0.2.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0464 calibrator=HistogramCalibrator quant)\n",
      "layers.13.layers.0.6.quantizer          : TensorQuantizer(8bit per-tensor amax=0.9965 calibrator=HistogramCalibrator quant)\n",
      "layers.13.layers.0.6.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0343 calibrator=HistogramCalibrator quant)\n",
      "layers.14.layers.0.2.quantizer          : TensorQuantizer(8bit per-tensor amax=1.3493 calibrator=HistogramCalibrator quant)\n",
      "layers.14.layers.0.2.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0396 calibrator=HistogramCalibrator quant)\n",
      "layers.14.layers.0.6.quantizer          : TensorQuantizer(8bit per-tensor amax=1.1563 calibrator=HistogramCalibrator quant)\n",
      "layers.14.layers.0.6.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0436 calibrator=HistogramCalibrator quant)\n",
      "end_layers.4.quantizer                  : TensorQuantizer(8bit per-tensor amax=4.0898 calibrator=HistogramCalibrator quant)\n",
      "end_layers.4.quantizer_w                : TensorQuantizer(8bit per-tensor amax=1.2548 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "\n",
    "     for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "         model(image.cpu())\n",
    "         if i >= num_batches:\n",
    "             break\n",
    "\n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cedaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.SDNs.wideresnet_sdn import WideResNet_SDN\n",
    "\n",
    "# torch.save(model.state_dict(), 'wideresnet_state_dict_sdnn.pth')\n",
    "\n",
    "# models = WideResNet_SDN()\n",
    "# torch.load('wideresnet_state_dict_sdnn.pth', map_location=\"cpu\")\n",
    "# # models = models.load_state_dict(torch.load('wideresnet_state_dict_sdnn.pth', map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2570157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timeit\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# model.eval()\n",
    "# start_time = timeit.default_timer()\n",
    "# with torch.no_grad():\n",
    "#     for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "#         images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "#         outputs = model(images)[-1]\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# print(timeit.default_timer() - start_time)\n",
    "# print('Accuracy of the network on the 10000 test images: %.4f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "851dddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the loaded model\n",
    "# # model.eval()\n",
    "# top1_test, top5_test, preds = fie.sdn_test_uncertainty(model, t_dataset.test_loader, \"cpu\")\n",
    "# print(\"Top-1 accuracy:\",top1_test)\n",
    "# print(\"Top-5 accuracy:\",top5_test)\n",
    "\n",
    "# # Sample prediction result\n",
    "# print(\"Correctly predicted?\",bool(preds[0][0]),\",Confidence:\",preds[0][1],\",Uncertainty:\",preds[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9687d24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72.7,\n",
       " 97.54,\n",
       " [7041, 886, 1465, 331, 193, 60, 0],\n",
       " [0, 0, 0, 0, 0, 0, 24],\n",
       " [2959, 2073, 608, 277, 84, 24, 4],\n",
       " [0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Test early exit capability of the mend model with zero uncertainty threshold and confidence threshold of 0.8\n",
    "# # uncertainty_threshold = -10\n",
    "# # confidence_threshold = 0.6\n",
    "\n",
    "uncertainty_threshold = 8\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "fie.sdn_test_early_exits(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecba4f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mul8s_1L2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2945e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def introduce_fault(model, percent_of_faults, fault_loc = None, layer_to_attack = None):\n",
    "    model.eval()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in layer_to_attack: \n",
    "        \n",
    "            print(\"Attacked layer\",name)\n",
    "            print(param.shape)\n",
    "            w1 = param.data\n",
    "            wf1 = torch.flatten(w1)\n",
    "            no_of_faults = int(percent_of_faults * len(wf1)/100)\n",
    "            if (no_of_faults > len(wf1)):\n",
    "                no_of_faults = len(wf1)\n",
    "\n",
    "            print(\"Number of weights attacked\", no_of_faults)\n",
    "            if fault_loc is None:\n",
    "                fault_loc = random.sample(range(0, len(wf1)), no_of_faults)\n",
    "                fault = [random.uniform(-2, 2) for _ in range(len(fault_loc))]\n",
    "#                 print(\"fault location\", fault)\n",
    "            \n",
    "            for i in range(0, len(fault_loc)):\n",
    "#                 print(f\"Fault values, before {wf1[fault_loc[i]]},   after: {-wf1[fault_loc[i]]}\")\n",
    "#                 wf1[fault_loc[i]] = -wf1[fault_loc[i]]\n",
    "                wf1[fault_loc[i]] = torch.tensor(fault[i])\n",
    "            \n",
    "            wf11 = wf1.reshape(w1.shape)\n",
    "            param.data = wf11\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1317f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attacked layer init_conv.weight\n",
      "torch.Size([16, 3, 3, 3])\n",
      "Number of weights attacked 129\n"
     ]
    }
   ],
   "source": [
    "# FP = ['layers.0.layers.1.0.weight'] # Example layers in vgg16\n",
    "# FP = ['layers.0.layers.1.0.weight','layers.0.layers.0.2.weight']# Example layers in wideresnet\n",
    "FP = [\"init_conv.weight\"] # Example layers in wideresnet\n",
    "FR = 30\n",
    "\n",
    "model = introduce_fault(model, FR, None, FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c46d653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault:  13.66 64.98 [9744, 173, 54, 14, 11, 2, 0] [0, 0, 0, 0, 0, 0, 2] [256, 83, 29, 15, 4, 2, 0] [0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault = \\\n",
    "  fie.sdn_test_early_exits(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")\n",
    "\n",
    "print(\"top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault: \",\n",
    "     top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af0d96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_quantization import nn as quant_nn\n",
    "# from pytorch_quantization import calib\n",
    "\n",
    "# def collect_stats(model, data_loader, num_batches):\n",
    "#      \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "#      # Enable calibrators\n",
    "#      for name, module in model.named_modules():\n",
    "#          if isinstance(module, quant_nn.TensorQuantizer):\n",
    "#              if module._calibrator is not None:\n",
    "#                  module.disable_quant()\n",
    "#                  module.enable_calib()\n",
    "#              else:\n",
    "#                  module.disable()\n",
    "\n",
    "#      for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "#          model(image.cpu())\n",
    "#          if i >= num_batches:\n",
    "#              break\n",
    "\n",
    "#      # Disable calibrators\n",
    "#      for name, module in model.named_modules():\n",
    "#          if isinstance(module, quant_nn.TensorQuantizer):\n",
    "#              if module._calibrator is not None:\n",
    "#                  module.enable_quant()\n",
    "#                  module.disable_calib()\n",
    "#              else:\n",
    "#                  module.enable()\n",
    "\n",
    "# def compute_amax(model, **kwargs):\n",
    "#  # Load calib result\n",
    "#  for name, module in model.named_modules():\n",
    "#      if isinstance(module, quant_nn.TensorQuantizer):\n",
    "#          if module._calibrator is not None:\n",
    "#              if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "#                  module.load_calib_amax()\n",
    "#              else:\n",
    "#                  module.load_calib_amax(**kwargs)\n",
    "#          print(F\"{name:40}: {module}\")\n",
    "#  model.cpu()\n",
    "\n",
    "# # It is a bit slow since we collect histograms on CPU\n",
    "# with torch.no_grad():\n",
    "#     stats = collect_stats(model, data_t, num_batches=2)\n",
    "#     amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "#     # optional - test different calibration methods\n",
    "#     #amax = compute_amax(model, method=\"mse\")\n",
    "#     #amax = compute_amax(model, method=\"entropy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "594fc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault = \\\n",
    "#   fie.sdn_test_early_exits(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")\n",
    "\n",
    "# print(\"top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault: \",\n",
    "#      top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
