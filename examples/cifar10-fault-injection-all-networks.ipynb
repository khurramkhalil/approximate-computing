{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a115dd",
   "metadata": {},
   "source": [
    "# Model evaluation and re-training with AdaPT on Cifar10 dataset\n",
    "\n",
    "In this notebook you can evaluate different approximate multipliers on various models based on Cifar10 dataset\n",
    "\n",
    "Steps:\n",
    "* Select models to load \n",
    "* Select number of threads to use\n",
    "* Choose approximate multiplier \n",
    "* Load model for evaluation\n",
    "* Load dataset\n",
    "* Run model calibration for quantization\n",
    "* Run model evaluation\n",
    "* Run approximate-aware re-training\n",
    "* Rerun model evaluation\n",
    "\n",
    "**Note**:\n",
    "* This notebook should be run on a X86 machine\n",
    "\n",
    "* Please make sure you have run the installation steps first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5eef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d4c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_seed():\n",
    "    return 1221 # 121 and 1221\n",
    "\n",
    "def set_random_seeds():\n",
    "    torch.manual_seed(get_random_seed())\n",
    "    np.random.seed(get_random_seed())\n",
    "    random.seed(get_random_seed())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab49b35",
   "metadata": {},
   "source": [
    "## Select models to load \n",
    "\n",
    "The weights must be downloaded in state_dicts folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183a13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.SDNs.vgg_sdn import vgg16_sdn_bn\n",
    "from models.SDNs.wideresnet_sdn import wideresnet_sdn_v1\n",
    "from models.SDNs.mobilenet_sdn import mobilenet_sdn_v1\n",
    "import models.SDNs.fault_injection as fie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69265983",
   "metadata": {},
   "source": [
    "## Select number of threads to use\n",
    "\n",
    "For optimal performance set them as the number of your cpu threads (not cpu cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165c2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_PLACES=cores\n",
      "env: OMP_PROC_BIND=close\n",
      "env: OMP_WAIT_POLICY=active\n"
     ]
    }
   ],
   "source": [
    "threads = 20\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "# maybe better performance\n",
    "%env OMP_PLACES=cores\n",
    "%env OMP_PROC_BIND=close\n",
    "%env OMP_WAIT_POLICY=active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa06300",
   "metadata": {},
   "source": [
    "## Choose approximate multiplier \n",
    "\n",
    "Two approximate multipliers are already provided\n",
    "\n",
    "**mul8s_acc** - (header file: mul8s_acc.h)   <--  default\n",
    "\n",
    "**mul8s_1L2H** - (header file: mul8s_1L2H.h)\n",
    "\n",
    "\n",
    "\n",
    "In order to use your custom multiplier you need to use the provided tool (LUT_generator) to easily create the C++ header for your multiplier. Then you just place it inside the adapt/cpu-kernels/axx_mults folder. The name of the axx_mult here must match the name of the header file. The same axx_mult is used in all layers. \n",
    "\n",
    "Tip: If you want explicitly to set for each layer a different axx_mult you must do it from the model definition using the respective AdaPT_Conv2d class of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562689c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "axx_mult = 'mul8s_acc'\n",
    "# axx_mult = 'mul8s_1L2H'\n",
    "\n",
    "# axx_mult = 'mul8s_1L2L'\n",
    "\n",
    "# axx_mult = 'mul8s_1L2N'\n",
    "# axx_mult = 'mul8s_1L12'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539e7e1",
   "metadata": {},
   "source": [
    "## Load model for evaluation\n",
    "\n",
    "Jit compilation method loads 'on the fly' the C++ extentions of the approximate multipliers. Then the pytorch model is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc26796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_mul8s_acc/build.ninja...\n",
      "Building extension module PyInit_conv2d_mul8s_acc...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_linear_mul8s_acc/build.ninja...\n",
      "Building extension module PyInit_linear_mul8s_acc...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNet_SDN(\n",
       "  (init_conv): Sequential(\n",
       "    (0): AdaPT_Conv2d(\n",
       "      3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    )\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (1): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (2): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (4): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (6): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (8): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (10): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (11): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (end_layers): Sequential(\n",
       "    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): AdaPT_Linear(\n",
       "      (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = vgg16_sdn_bn(pretrained=True, axx_mult = axx_mult)\n",
    "# model = wideresnet_sdn_v1(pretrained=True, axx_mult = axx_mult)\n",
    "model = mobilenet_sdn_v1(pretrained=True, axx_mult = axx_mult)\n",
    "\n",
    "model.eval() # for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de58a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "init_conv\n",
      "init_conv.0\n",
      "init_conv.0.quantizer\n",
      "init_conv.0.quantizer_w\n",
      "init_conv.1\n",
      "init_conv.2\n",
      "layers\n",
      "layers.0\n",
      "layers.0.layers\n",
      "layers.0.layers.0\n",
      "layers.0.layers.0.quantizer\n",
      "layers.0.layers.0.quantizer_w\n",
      "layers.0.layers.1\n",
      "layers.0.layers.2\n",
      "layers.0.layers.3\n",
      "layers.0.layers.3.quantizer\n",
      "layers.0.layers.3.quantizer_w\n",
      "layers.0.layers.4\n",
      "layers.0.layers.5\n",
      "layers.0.output\n",
      "layers.1\n",
      "layers.1.layers\n",
      "layers.1.layers.0\n",
      "layers.1.layers.0.quantizer\n",
      "layers.1.layers.0.quantizer_w\n",
      "layers.1.layers.1\n",
      "layers.1.layers.2\n",
      "layers.1.layers.3\n",
      "layers.1.layers.3.quantizer\n",
      "layers.1.layers.3.quantizer_w\n",
      "layers.1.layers.4\n",
      "layers.1.layers.5\n",
      "layers.1.output\n",
      "layers.2\n",
      "layers.2.layers\n",
      "layers.2.layers.0\n",
      "layers.2.layers.0.quantizer\n",
      "layers.2.layers.0.quantizer_w\n",
      "layers.2.layers.1\n",
      "layers.2.layers.2\n",
      "layers.2.layers.3\n",
      "layers.2.layers.3.quantizer\n",
      "layers.2.layers.3.quantizer_w\n",
      "layers.2.layers.4\n",
      "layers.2.layers.5\n",
      "layers.2.output\n",
      "layers.2.output.max_pool\n",
      "layers.2.output.avg_pool\n",
      "layers.2.output.linear\n",
      "layers.2.output.linear.quantizer\n",
      "layers.2.output.linear.quantizer_w\n",
      "layers.3\n",
      "layers.3.layers\n",
      "layers.3.layers.0\n",
      "layers.3.layers.0.quantizer\n",
      "layers.3.layers.0.quantizer_w\n",
      "layers.3.layers.1\n",
      "layers.3.layers.2\n",
      "layers.3.layers.3\n",
      "layers.3.layers.3.quantizer\n",
      "layers.3.layers.3.quantizer_w\n",
      "layers.3.layers.4\n",
      "layers.3.layers.5\n",
      "layers.3.output\n",
      "layers.4\n",
      "layers.4.layers\n",
      "layers.4.layers.0\n",
      "layers.4.layers.0.quantizer\n",
      "layers.4.layers.0.quantizer_w\n",
      "layers.4.layers.1\n",
      "layers.4.layers.2\n",
      "layers.4.layers.3\n",
      "layers.4.layers.3.quantizer\n",
      "layers.4.layers.3.quantizer_w\n",
      "layers.4.layers.4\n",
      "layers.4.layers.5\n",
      "layers.4.output\n",
      "layers.4.output.max_pool\n",
      "layers.4.output.avg_pool\n",
      "layers.4.output.linear\n",
      "layers.4.output.linear.quantizer\n",
      "layers.4.output.linear.quantizer_w\n",
      "layers.5\n",
      "layers.5.layers\n",
      "layers.5.layers.0\n",
      "layers.5.layers.0.quantizer\n",
      "layers.5.layers.0.quantizer_w\n",
      "layers.5.layers.1\n",
      "layers.5.layers.2\n",
      "layers.5.layers.3\n",
      "layers.5.layers.3.quantizer\n",
      "layers.5.layers.3.quantizer_w\n",
      "layers.5.layers.4\n",
      "layers.5.layers.5\n",
      "layers.5.output\n",
      "layers.6\n",
      "layers.6.layers\n",
      "layers.6.layers.0\n",
      "layers.6.layers.0.quantizer\n",
      "layers.6.layers.0.quantizer_w\n",
      "layers.6.layers.1\n",
      "layers.6.layers.2\n",
      "layers.6.layers.3\n",
      "layers.6.layers.3.quantizer\n",
      "layers.6.layers.3.quantizer_w\n",
      "layers.6.layers.4\n",
      "layers.6.layers.5\n",
      "layers.6.output\n",
      "layers.6.output.max_pool\n",
      "layers.6.output.avg_pool\n",
      "layers.6.output.linear\n",
      "layers.6.output.linear.quantizer\n",
      "layers.6.output.linear.quantizer_w\n",
      "layers.7\n",
      "layers.7.layers\n",
      "layers.7.layers.0\n",
      "layers.7.layers.0.quantizer\n",
      "layers.7.layers.0.quantizer_w\n",
      "layers.7.layers.1\n",
      "layers.7.layers.2\n",
      "layers.7.layers.3\n",
      "layers.7.layers.3.quantizer\n",
      "layers.7.layers.3.quantizer_w\n",
      "layers.7.layers.4\n",
      "layers.7.layers.5\n",
      "layers.7.output\n",
      "layers.8\n",
      "layers.8.layers\n",
      "layers.8.layers.0\n",
      "layers.8.layers.0.quantizer\n",
      "layers.8.layers.0.quantizer_w\n",
      "layers.8.layers.1\n",
      "layers.8.layers.2\n",
      "layers.8.layers.3\n",
      "layers.8.layers.3.quantizer\n",
      "layers.8.layers.3.quantizer_w\n",
      "layers.8.layers.4\n",
      "layers.8.layers.5\n",
      "layers.8.output\n",
      "layers.8.output.max_pool\n",
      "layers.8.output.avg_pool\n",
      "layers.8.output.linear\n",
      "layers.8.output.linear.quantizer\n",
      "layers.8.output.linear.quantizer_w\n",
      "layers.9\n",
      "layers.9.layers\n",
      "layers.9.layers.0\n",
      "layers.9.layers.0.quantizer\n",
      "layers.9.layers.0.quantizer_w\n",
      "layers.9.layers.1\n",
      "layers.9.layers.2\n",
      "layers.9.layers.3\n",
      "layers.9.layers.3.quantizer\n",
      "layers.9.layers.3.quantizer_w\n",
      "layers.9.layers.4\n",
      "layers.9.layers.5\n",
      "layers.9.output\n",
      "layers.10\n",
      "layers.10.layers\n",
      "layers.10.layers.0\n",
      "layers.10.layers.0.quantizer\n",
      "layers.10.layers.0.quantizer_w\n",
      "layers.10.layers.1\n",
      "layers.10.layers.2\n",
      "layers.10.layers.3\n",
      "layers.10.layers.3.quantizer\n",
      "layers.10.layers.3.quantizer_w\n",
      "layers.10.layers.4\n",
      "layers.10.layers.5\n",
      "layers.10.output\n",
      "layers.11\n",
      "layers.11.layers\n",
      "layers.11.layers.0\n",
      "layers.11.layers.0.quantizer\n",
      "layers.11.layers.0.quantizer_w\n",
      "layers.11.layers.1\n",
      "layers.11.layers.2\n",
      "layers.11.layers.3\n",
      "layers.11.layers.3.quantizer\n",
      "layers.11.layers.3.quantizer_w\n",
      "layers.11.layers.4\n",
      "layers.11.layers.5\n",
      "layers.11.output\n",
      "layers.11.output.linear\n",
      "layers.11.output.linear.quantizer\n",
      "layers.11.output.linear.quantizer_w\n",
      "layers.12\n",
      "layers.12.layers\n",
      "layers.12.layers.0\n",
      "layers.12.layers.0.quantizer\n",
      "layers.12.layers.0.quantizer_w\n",
      "layers.12.layers.1\n",
      "layers.12.layers.2\n",
      "layers.12.layers.3\n",
      "layers.12.layers.3.quantizer\n",
      "layers.12.layers.3.quantizer_w\n",
      "layers.12.layers.4\n",
      "layers.12.layers.5\n",
      "layers.12.output\n",
      "end_layers\n",
      "end_layers.0\n",
      "end_layers.1\n",
      "end_layers.2\n",
      "end_layers.2.quantizer\n",
      "end_layers.2.quantizer_w\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "set_random_seeds()\n",
    "\n",
    "# Print names of immediate layers only\n",
    "for name, layer in model.named_modules():\n",
    "    print(name)\n",
    "    \n",
    "# layers.0.layers.0.quantizer\n",
    "# layers.0.layers.0.quantizer_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76721ed0",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63b4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128,\n",
    "                                            shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47aa7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTrigger(object):\n",
    "    def __init__(self, square_size=5, square_loc=(26,26)):\n",
    "        self.square_size = square_size\n",
    "        self.square_loc = square_loc\n",
    "\n",
    "    def __call__(self, pil_data):\n",
    "        square = Image.new('L', (self.square_size, self.square_size), 255)\n",
    "        pil_data.paste(square, self.square_loc)\n",
    "        return pil_data\n",
    "\n",
    "class Cifar10_:\n",
    "    def __init__(self, batch_size=128, add_trigger=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = 32\n",
    "        self.num_classes = 10\n",
    "        self.num_test = 10000\n",
    "        self.num_train = 50000\n",
    "\n",
    "        normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.augmented = T.Compose([T.RandomHorizontalFlip(), T.RandomCrop(32, padding=4),T.ToTensor(), normalize])\n",
    "\n",
    "        self.normalized = T.Compose([T.ToTensor(), normalize])\n",
    "\n",
    "        self.aug_trainset =  CIFAR10(root='datasets/cifar10_data', train=True, download=False, transform=self.augmented)\n",
    "        self.aug_train_loader = torch.utils.data.DataLoader(self.aug_trainset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        self.trainset =  CIFAR10(root='datasets/cifar10_data', train=True, download=False, transform=self.normalized)\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.testset =  CIFAR10(root='datasets/cifar10_data', train=False, download=False, transform=self.normalized)\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        # add trigger to the test set samples\n",
    "        # for the experiments on the backdoored CNNs and SDNs\n",
    "        #  uncomment third line to measure backdoor attack success, right now it measures standard accuracy\n",
    "        if add_trigger: \n",
    "            self.trigger_transform = T.Compose([AddTrigger(), T.ToTensor(), normalize])\n",
    "            self.trigger_test_set = CIFAR10(root='datasets/cifar10_data', train=False, download=False, transform=self.trigger_transform)\n",
    "            # self.trigger_test_set.test_labels = [5] * self.num_test\n",
    "            self.trigger_test_loader = torch.utils.data.DataLoader(self.trigger_test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "def load_cifar10(batch_size, add_trigger=False):\n",
    "    cifar10_data = Cifar10_(batch_size=batch_size, add_trigger=add_trigger)\n",
    "    return cifar10_data\n",
    "\n",
    "def get_dataset(batch_size=128, add_trigger=False):\n",
    "    return load_cifar10(batch_size, add_trigger)\n",
    "\n",
    "t_dataset = get_dataset()\n",
    "one_batch_dataset = get_dataset(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa74c5d",
   "metadata": {},
   "source": [
    "## Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters \n",
    "\n",
    "Need to re-run it each time the model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "946f0d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.81s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0905 21:44:42.301057 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.301445 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.301767 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.302050 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.302324 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.303059 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.303351 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.303630 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.303910 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.304197 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.304496 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.304785 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.305063 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.305333 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.305619 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.305893 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.306181 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.306458 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.306734 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.307011 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.307295 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.307567 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.307862 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.308134 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.308477 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.308814 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.309161 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.309500 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.309844 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.310176 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.310512 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.310818 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.311212 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.313082 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.313374 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.313641 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.313909 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.314166 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.314409 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.314708 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.315009 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.315294 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.315550 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.315797 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.316060 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.316310 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.316560 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.316865 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.317204 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.317462 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.317722 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.317966 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.318202 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.318434 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.318675 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.318907 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.319152 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.319395 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.319656 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.319885 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.320130 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.320418 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.320669 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.320914 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.321252 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.321539 140100394760000 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0905 21:44:42.322335 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.324016 140100394760000 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0905 21:44:42.324615 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.324970 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.325356 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.326012 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.326356 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.326928 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.327382 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.327854 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.328192 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.328761 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.329223 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.329689 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.330024 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.330588 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.331065 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.331416 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.331989 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0905 21:44:42.332457 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.332910 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.333283 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.333626 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.334268 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.334599 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.335180 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.335521 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.336082 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.336542 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.337003 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.337342 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.337693 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.338037 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.338775 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.339124 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.339694 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.340033 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.340594 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.340940 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.341560 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.341906 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.342468 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.342924 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.343382 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.343718 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.344269 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.344620 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.345186 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.345518 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.346069 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.346398 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.346949 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.347396 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.347856 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.348306 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.348759 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.349093 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.349649 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.350093 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.350548 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.350869 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.351419 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.351863 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.352319 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.352764 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.353221 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0905 21:44:42.353660 140100394760000 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_conv.0.quantizer                   : TensorQuantizer(8bit per-tensor amax=2.6387 calibrator=HistogramCalibrator quant)\n",
      "init_conv.0.quantizer_w                 : TensorQuantizer(8bit per-tensor amax=1.9424 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=5.4440 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=2.1052 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=7.0419 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=1.8036 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=4.5778 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.9850 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=5.0728 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=1.1897 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=2.4144 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=1.6961 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=4.0416 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.9056 calibrator=HistogramCalibrator quant)\n",
      "layers.2.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=2.6223 calibrator=HistogramCalibrator quant)\n",
      "layers.2.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.8813 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=2.2764 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.5959 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=3.9262 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.5896 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=1.8202 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.7298 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=2.9744 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.5165 calibrator=HistogramCalibrator quant)\n",
      "layers.4.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=2.1146 calibrator=HistogramCalibrator quant)\n",
      "layers.4.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.4239 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=1.6127 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.4781 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=3.9200 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.3843 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=1.5415 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.5861 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=2.7038 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.2957 calibrator=HistogramCalibrator quant)\n",
      "layers.6.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=1.3610 calibrator=HistogramCalibrator quant)\n",
      "layers.6.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.4955 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=1.3610 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.4729 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=2.8719 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.2594 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=1.3742 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.4157 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=2.6949 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.1914 calibrator=HistogramCalibrator quant)\n",
      "layers.8.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=1.5367 calibrator=HistogramCalibrator quant)\n",
      "layers.8.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.3293 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=1.5367 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.2907 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=2.6914 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.1446 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.quantizer            : TensorQuantizer(8bit per-tensor amax=1.8902 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.2419 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.3.quantizer            : TensorQuantizer(8bit per-tensor amax=2.4278 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.3.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.1370 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.quantizer            : TensorQuantizer(8bit per-tensor amax=2.0860 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.2298 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.3.quantizer            : TensorQuantizer(8bit per-tensor amax=2.3625 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.3.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.1134 calibrator=HistogramCalibrator quant)\n",
      "layers.11.output.linear.quantizer       : TensorQuantizer(8bit per-tensor amax=1.8897 calibrator=HistogramCalibrator quant)\n",
      "layers.11.output.linear.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.1626 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.quantizer            : TensorQuantizer(8bit per-tensor amax=1.8897 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.1916 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.3.quantizer            : TensorQuantizer(8bit per-tensor amax=1.7752 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.3.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.0784 calibrator=HistogramCalibrator quant)\n",
      "end_layers.2.quantizer                  : TensorQuantizer(8bit per-tensor amax=1.1119 calibrator=HistogramCalibrator quant)\n",
      "end_layers.2.quantizer_w                : TensorQuantizer(8bit per-tensor amax=0.4073 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "\n",
    "     for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "         model(image.cpu())\n",
    "         if i >= num_batches:\n",
    "             break\n",
    "\n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cedaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.SDNs.wideresnet_sdn import WideResNet_SDN\n",
    "\n",
    "# torch.save(model.state_dict(), 'wideresnet_state_dict_sdnn.pth')\n",
    "\n",
    "# models = WideResNet_SDN()\n",
    "# torch.load('wideresnet_state_dict_sdnn.pth', map_location=\"cpu\")\n",
    "# # models = models.load_state_dict(torch.load('wideresnet_state_dict_sdnn.pth', map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2570157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timeit\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# model.eval()\n",
    "# start_time = timeit.default_timer()\n",
    "# with torch.no_grad():\n",
    "#     for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "#         images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "#         outputs = model(images)[-1]\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# print(timeit.default_timer() - start_time)\n",
    "# print('Accuracy of the network on the 10000 test images: %.4f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "851dddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the loaded model\n",
    "# # model.eval()\n",
    "# top1_test, top5_test, preds = fie.sdn_test_uncertainty(model, t_dataset.test_loader, \"cpu\")\n",
    "# print(\"Top-1 accuracy:\",top1_test)\n",
    "# print(\"Top-5 accuracy:\",top5_test)\n",
    "\n",
    "# # Sample prediction result\n",
    "# print(\"Correctly predicted?\",bool(preds[0][0]),\",Confidence:\",preds[0][1],\",Uncertainty:\",preds[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9687d24e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m uncertainty_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      6\u001b[0m confidence_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mfie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msdn_test_early_exits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_batch_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muncertainty_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/adapt/examples/models/SDNs/fault_injection.py:201\u001b[0m, in \u001b[0;36msdn_test_early_exits\u001b[0;34m(model, loader, confidence_threshold, uncertainty_threshold, device)\u001b[0m\n\u001b[1;32m    199\u001b[0m b_x \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    200\u001b[0m b_y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 201\u001b[0m output, output_id, is_early, violations \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_early:\n\u001b[1;32m    205\u001b[0m     early_output_counts[output_id] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/workspace/adapt/examples/models/SDNs/mobilenet_sdn.py:147\u001b[0m, in \u001b[0;36mMobileNet_SDN.early_exit\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    145\u001b[0m output_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 147\u001b[0m     fwd, is_output, output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfwd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_output:\n\u001b[1;32m    150\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mappend(output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/workspace/adapt/examples/models/SDNs/mobilenet_sdn.py:57\u001b[0m, in \u001b[0;36mBlockWOutput.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 57\u001b[0m     fwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fwd, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(fwd)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:119\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[0;32m/workspace/adapt/adapt/approx_layers/axx_layers.py:232\u001b[0m, in \u001b[0;36mAdaPT_Conv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/adapt/adapt/approx_layers/axx_layers.py:229\u001b[0m, in \u001b[0;36mAdaPT_Conv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conv_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):       \n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAdaPT_Conv2d_Function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer_w\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mamax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxx_conv2d_kernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/adapt/adapt/approx_layers/axx_layers.py:140\u001b[0m, in \u001b[0;36mAdaPT_Conv2d_Function.forward\u001b[0;34m(ctx, input, weight, quantizer, quantizer_w, kernel_size, amax, amax_w, max_value, out_channels, bias_, axx_conv2d_kernel, bias, stride, padding, dilation, groups, padding_mode)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,groups):\n\u001b[1;32m    139\u001b[0m     filters \u001b[38;5;241m=\u001b[39m quant_weight[i:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)]                   \n\u001b[0;32m--> 140\u001b[0m     o \u001b[38;5;241m=\u001b[39m  \u001b[43maxx_conv2d_kernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    141\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((out, o), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    143\u001b[0m out \u001b[38;5;241m=\u001b[39m (out\u001b[38;5;241m/\u001b[39m((max_value\u001b[38;5;241m/\u001b[39mamax)\u001b[38;5;241m*\u001b[39m((max_value\u001b[38;5;241m/\u001b[39mamax_w))))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Test early exit capability of the mend model with zero uncertainty threshold and confidence threshold of 0.8\n",
    "# # uncertainty_threshold = -10\n",
    "# # confidence_threshold = 0.6\n",
    "\n",
    "uncertainty_threshold = 8\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "fie.sdn_test_early_exits(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba4f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mul8s_1L2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2945e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def introduce_fault(model, percent_of_faults, fault_loc = None, layer_to_attack = None):\n",
    "    model.eval()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in layer_to_attack: \n",
    "        \n",
    "            print(\"Attacked layer\",name)\n",
    "            print(param.shape)\n",
    "            w1 = param.data\n",
    "            wf1 = torch.flatten(w1)\n",
    "            no_of_faults = int(percent_of_faults * len(wf1)/100)\n",
    "            if (no_of_faults > len(wf1)):\n",
    "                no_of_faults = len(wf1)\n",
    "\n",
    "            print(\"Number of weights attacked\", no_of_faults)\n",
    "            if fault_loc is None:\n",
    "                fault_loc = random.sample(range(0, len(wf1)), no_of_faults)\n",
    "                fault = [random.uniform(-2, 2) for _ in range(len(fault_loc))]\n",
    "#                 print(\"fault location\", fault)\n",
    "            \n",
    "            for i in range(0, len(fault_loc)):\n",
    "#                 print(f\"Fault values, before {wf1[fault_loc[i]]},   after: {-wf1[fault_loc[i]]}\")\n",
    "#                 wf1[fault_loc[i]] = -wf1[fault_loc[i]]\n",
    "                wf1[fault_loc[i]] = torch.tensor(fault[i])\n",
    "            \n",
    "            wf11 = wf1.reshape(w1.shape)\n",
    "            param.data = wf11\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP = ['layers.0.layers.1.0.weight'] # Example layers in vgg16\n",
    "# FP = ['layers.0.layers.1.0.weight','layers.0.layers.0.2.weight']# Example layers in wideresnet\n",
    "FP = [\"init_conv.weight\"] # Example layers in wideresnet\n",
    "FR = 30\n",
    "\n",
    "model = introduce_fault(model, FR, None, FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault = \\\n",
    "  fie.sdn_test_early_exits(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")\n",
    "\n",
    "print(\"top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault: \",\n",
    "     top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0d96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_quantization import nn as quant_nn\n",
    "# from pytorch_quantization import calib\n",
    "\n",
    "# def collect_stats(model, data_loader, num_batches):\n",
    "#      \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "#      # Enable calibrators\n",
    "#      for name, module in model.named_modules():\n",
    "#          if isinstance(module, quant_nn.TensorQuantizer):\n",
    "#              if module._calibrator is not None:\n",
    "#                  module.disable_quant()\n",
    "#                  module.enable_calib()\n",
    "#              else:\n",
    "#                  module.disable()\n",
    "\n",
    "#      for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "#          model(image.cpu())\n",
    "#          if i >= num_batches:\n",
    "#              break\n",
    "\n",
    "#      # Disable calibrators\n",
    "#      for name, module in model.named_modules():\n",
    "#          if isinstance(module, quant_nn.TensorQuantizer):\n",
    "#              if module._calibrator is not None:\n",
    "#                  module.enable_quant()\n",
    "#                  module.disable_calib()\n",
    "#              else:\n",
    "#                  module.enable()\n",
    "\n",
    "# def compute_amax(model, **kwargs):\n",
    "#  # Load calib result\n",
    "#  for name, module in model.named_modules():\n",
    "#      if isinstance(module, quant_nn.TensorQuantizer):\n",
    "#          if module._calibrator is not None:\n",
    "#              if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "#                  module.load_calib_amax()\n",
    "#              else:\n",
    "#                  module.load_calib_amax(**kwargs)\n",
    "#          print(F\"{name:40}: {module}\")\n",
    "#  model.cpu()\n",
    "\n",
    "# # It is a bit slow since we collect histograms on CPU\n",
    "# with torch.no_grad():\n",
    "#     stats = collect_stats(model, data_t, num_batches=2)\n",
    "#     amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "#     # optional - test different calibration methods\n",
    "#     #amax = compute_amax(model, method=\"mse\")\n",
    "#     #amax = compute_amax(model, method=\"entropy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594fc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault = \\\n",
    "#   fie.sdn_test_early_exits(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")\n",
    "\n",
    "# print(\"top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault: \",\n",
    "#      top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
