{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a115dd",
   "metadata": {},
   "source": [
    "# Model evaluation and re-training with AdaPT on Cifar10 dataset\n",
    "\n",
    "In this notebook you can evaluate different approximate multipliers on various models based on Cifar10 dataset\n",
    "\n",
    "Steps:\n",
    "* Select models to load \n",
    "* Select number of threads to use\n",
    "* Choose approximate multiplier \n",
    "* Load model for evaluation\n",
    "* Load dataset\n",
    "* Run model calibration for quantization\n",
    "* Run model evaluation\n",
    "* Run approximate-aware re-training\n",
    "* Rerun model evaluation\n",
    "\n",
    "**Note**:\n",
    "* This notebook should be run on a X86 machine\n",
    "\n",
    "* Please make sure you have run the installation steps first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5eef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10, CIFAR100, MNIST\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d4c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_seed():\n",
    "    return 1221 # 121 and 1221\n",
    "\n",
    "def set_random_seeds():\n",
    "    torch.manual_seed(get_random_seed())\n",
    "    np.random.seed(get_random_seed())\n",
    "    random.seed(get_random_seed())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab49b35",
   "metadata": {},
   "source": [
    "## Select models to load \n",
    "\n",
    "The weights must be downloaded in state_dicts folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183a13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.SDNs.vgg_sdn import vgg16_sdn_bn\n",
    "from models.SDNs.wideresnet_sdn import wideresnet_sdn_v1\n",
    "from models.SDNs.mobilenet_sdn import mobilenet_sdn_v1\n",
    "import models.SDNs.fault_injection as fie\n",
    "import models.SDNs.sdm_fault_mitigation as sdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69265983",
   "metadata": {},
   "source": [
    "## Select number of threads to use\n",
    "\n",
    "For optimal performance set them as the number of your cpu threads (not cpu cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165c2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_PLACES=cores\n",
      "env: OMP_PROC_BIND=close\n",
      "env: OMP_WAIT_POLICY=active\n"
     ]
    }
   ],
   "source": [
    "threads = 20\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "# maybe better performance\n",
    "%env OMP_PLACES=cores\n",
    "%env OMP_PROC_BIND=close\n",
    "%env OMP_WAIT_POLICY=active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa06300",
   "metadata": {},
   "source": [
    "## Choose approximate multiplier \n",
    "\n",
    "Two approximate multipliers are already provided\n",
    "\n",
    "**mul8s_acc** - (header file: mul8s_acc.h)   <--  default\n",
    "\n",
    "**mul8s_1L2H** - (header file: mul8s_1L2H.h)\n",
    "\n",
    "\n",
    "\n",
    "In order to use your custom multiplier you need to use the provided tool (LUT_generator) to easily create the C++ header for your multiplier. Then you just place it inside the adapt/cpu-kernels/axx_mults folder. The name of the axx_mult here must match the name of the header file. The same axx_mult is used in all layers. \n",
    "\n",
    "Tip: If you want explicitly to set for each layer a different axx_mult you must do it from the model definition using the respective AdaPT_Conv2d class of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562689c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axx_mult = 'mul8s_acc'\n",
    "axx_mult = 'mul8s_1KV6'\n",
    "# axx_mult = 'mul8s_1KV8'\n",
    "# axx_mult = 'mul8s_1KV9'\n",
    "# axx_mult = 'mul8s_1KVP'\n",
    "# axx_mult = 'mul8s_1L2J'\n",
    "# axx_mult = 'mul8s_1L2H'\n",
    "# axx_mult = 'mul8s_1L2N'\n",
    "# axx_mult = 'mul8s_1L12'\n",
    "\n",
    "# dataset_name = \"CIFAR10\"\n",
    "dataset_name = \"mnist\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539e7e1",
   "metadata": {},
   "source": [
    "## Load model for evaluation\n",
    "\n",
    "Jit compilation method loads 'on the fly' the C++ extentions of the approximate multipliers. Then the pytorch model is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc26796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_mul8s_1KV6/build.ninja...\n",
      "Building extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1KV6, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1KV6...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MobileNet_SDN(\n",
       "  (init_conv): Sequential(\n",
       "    (0): AdaPT_Conv2d(\n",
       "      1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
       "      (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    )\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (1): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (2): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "        (linear): Linear(in_features=2048, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (4): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (linear): Linear(in_features=4096, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (5): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (6): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "        (linear): Linear(in_features=8192, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (7): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (8): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "        (linear): Linear(in_features=8192, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (9): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (10): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "    (11): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (linear): Linear(in_features=4096, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (12): BlockWOutput(\n",
       "      (layers): Sequential(\n",
       "        (0): AdaPT_Conv2d(\n",
       "          1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): AdaPT_Conv2d(\n",
       "          1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "        (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU()\n",
       "      )\n",
       "      (output): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (end_layers): Sequential(\n",
       "    (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = vgg16_sdn_bn(pretrained=True, axx_mult = axx_mult, dataset_name=dataset_name)\n",
    "# model = wideresnet_sdn_v1(pretrained=True, axx_mult = axx_mult, dataset_name=dataset_name)\n",
    "model = mobilenet_sdn_v1(pretrained=True, axx_mult = axx_mult, dataset_name=dataset_name)\n",
    "\n",
    "model.eval() # for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de58a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "set_random_seeds()\n",
    "\n",
    "# original_parameter = deepcopy(dict(model.named_parameters())['layers.0.layers.0.weight'])\n",
    "\n",
    "# # Print names of immediate layers only\n",
    "# for name, layer in model.named_modules():\n",
    "#     print(name)\n",
    "    \n",
    "# layers.0.layers.0.quantizer\n",
    "# layers.0.layers.0.quantizer_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef12a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set random seeds\n",
    "# set_random_seeds()\n",
    "# from scipy import stats\n",
    "# from adapt.approx_layers import axx_layers as approxNN\n",
    "\n",
    "# check_layer = \"init_conv.weight\"\n",
    "# initial_stats = {}\n",
    "# # Print names of immediate layers only\n",
    "# for name, module in model.named_modules():\n",
    "#     print(name)\n",
    "#     if isinstance(module, approxNN.AdaPT_Conv2d) and name in check_layer:\n",
    "#         print(\"This\")\n",
    "#         weights = module.weight.data.cpu().numpy().flatten()\n",
    "#         initial_stats[name] = {\n",
    "#             'mean': np.mean(weights),\n",
    "#             'std': np.std(weights),\n",
    "#             'sample': np.random.choice(weights, size=100, replace=False)\n",
    "#         }\n",
    "#         print(\"length: \", len(weights))\n",
    "#         break\n",
    "\n",
    "\n",
    "# module = dict(model.named_modules())[name]\n",
    "# current_weights = module.weight.data.cpu().numpy().flatten()\n",
    "# current_sample = np.random.choice(current_weights, size=100, replace=False)\n",
    "\n",
    "# ks_statistic, p_value = stats.ks_2samp(initial_stats[name]['sample'], current_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76721ed0",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f63b4701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "#     transform = T.Compose(\n",
    "#         [\n",
    "#             T.ToTensor(),\n",
    "#             T.Normalize(mean, std),\n",
    "#         ]\n",
    "#     )\n",
    "#     dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "#     dataloader = DataLoader(\n",
    "#         dataset,\n",
    "#         batch_size=128,\n",
    "#         num_workers=0,\n",
    "#         drop_last=True,\n",
    "#         pin_memory=False,\n",
    "#     )\n",
    "#     return dataloader\n",
    "\n",
    "# transform = T.Compose(\n",
    "#         [\n",
    "#             T.RandomCrop(32, padding=4),\n",
    "#             T.RandomHorizontalFlip(),\n",
    "#             T.ToTensor(),\n",
    "#             T.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),\n",
    "#         ]\n",
    "#     )\n",
    "# dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "# evens = list(range(0, len(dataset), 10))\n",
    "# trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "# data = val_dataloader()\n",
    "\n",
    "# # data_t is used for calibration purposes and is a subset of train-set\n",
    "# data_t = DataLoader(trainset_1, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms as T\n",
    "\n",
    "def val_dataloader(mean=(0.1307,), std=(0.3081,)):\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = datasets.MNIST(root=\"datasets/mnist_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "    [   T.Resize((32, 32)),\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=(0.1307,), std=(0.3081,)),\n",
    "    ]\n",
    ")\n",
    "dataset = datasets.MNIST(root=\"datasets/mnist_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "# data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47aa7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10_:\n",
    "    def __init__(self, batch_size=128, add_trigger=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = 32\n",
    "        self.num_classes = 10\n",
    "        self.num_test = 100  # Set the number of test samples to 2000\n",
    "        self.num_train = 50000\n",
    "\n",
    "        normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.augmented = T.Compose([T.RandomHorizontalFlip(), T.RandomCrop(32, padding=4), T.ToTensor(), normalize])\n",
    "        self.normalized = T.Compose([T.ToTensor(), normalize])\n",
    "\n",
    "        self.aug_trainset = CIFAR10(root='datasets/cifar10_data', train=True, download=False, transform=self.augmented)\n",
    "        self.aug_train_loader = torch.utils.data.DataLoader(self.aug_trainset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        self.trainset = CIFAR10(root='datasets/cifar10_data', train=True, download=False, transform=self.normalized)\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Load the full test set\n",
    "        self.full_testset = CIFAR10(root='datasets/cifar10_data', train=False, download=False, transform=self.normalized)\n",
    "        \n",
    "        indices = np.random.choice(len(self.full_testset), self.num_test, replace=False)\n",
    "        \n",
    "        # Create a subset of the test set with these indices\n",
    "        self.testset = torch.utils.data.Subset(self.full_testset, indices)\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "class Cifar100_:\n",
    "    def __init__(self, batch_size=128, add_trigger=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = 32\n",
    "        self.num_classes = 10\n",
    "        self.num_test = 10000  # Set the number of test samples to 2000\n",
    "        self.num_train = 50000\n",
    "\n",
    "        normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.augmented = T.Compose([T.RandomHorizontalFlip(), T.RandomCrop(32, padding=4), T.ToTensor(), normalize])\n",
    "        self.normalized = T.Compose([T.ToTensor(), normalize])\n",
    "\n",
    "        self.aug_trainset = CIFAR100(root='datasets/cifar100_data', train=True, download=True, transform=self.augmented)\n",
    "        self.aug_train_loader = torch.utils.data.DataLoader(self.aug_trainset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        self.trainset = CIFAR100(root='datasets/cifar100_data', train=True, download=True, transform=self.normalized)\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Load the full test set\n",
    "        self.full_testset = CIFAR100(root='datasets/cifar100_data', train=False, download=True, transform=self.normalized)\n",
    "        \n",
    "        indices = np.random.choice(len(self.full_testset), self.num_test, replace=False)\n",
    "        \n",
    "        # Create a subset of the test set with these indices\n",
    "        self.testset = torch.utils.data.Subset(self.full_testset, indices)\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        \n",
    "class MNIST:\n",
    "    def __init__(self, batch_size=128):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = 32  # Image size for MNIST is 28x28\n",
    "        self.num_classes = 10  # MNIST has 10 classes\n",
    "        self.num_test = 10000  # MNIST test set size\n",
    "        self.num_train = 60000  # MNIST training set size\n",
    "    \n",
    "        # Normalization parameters for MNIST (mean, std)\n",
    "        normalize = T.Normalize(mean=[0.5], std=[0.5])\n",
    "        \n",
    "        # Data augmentation for MNIST\n",
    "        self.augmented = T.Compose([\n",
    "            T.Resize((32, 32)),  # Resize to 32x32\n",
    "            T.RandomHorizontalFlip(), \n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.ToTensor(), \n",
    "            normalize\n",
    "        ])\n",
    "        \n",
    "        # Normalized transformation (no augmentation)\n",
    "        self.normalized = T.Compose([\n",
    "            T.Resize((32, 32)),  # Resize to 32x32\n",
    "            T.ToTensor(), \n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "        # Augmented training set\n",
    "        self.aug_trainset = datasets.MNIST(root='datasets/mnist_data', train=True, download=True, transform=self.augmented)\n",
    "        self.aug_train_loader = torch.utils.data.DataLoader(self.aug_trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        # Non-augmented training set\n",
    "        self.trainset = datasets.MNIST(root='datasets/mnist_data', train=True, download=True, transform=self.normalized)\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "        # Test set\n",
    "        self.testset = datasets.MNIST(root='datasets/mnist_data', train=False, download=True, transform=self.normalized)\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "        \n",
    "        \n",
    "def load_dataset(batch_size, add_trigger=False, dataset_name=\"CIFAR10\"):\n",
    "    if dataset_name == \"CIFAR10\":\n",
    "        data = Cifar10_(batch_size=batch_size, add_trigger=add_trigger)\n",
    "    elif dataset_name == \"CIFAR100\":\n",
    "        data = Cifar100_(batch_size=batch_size, add_trigger=add_trigger)\n",
    "    elif dataset_name == \"mnist\":\n",
    "        data = MNIST(batch_size=batch_size)\n",
    "    return data\n",
    "\n",
    "def get_dataset(batch_size=128, add_trigger=False, dataset_name=\"CIFAR10\"):\n",
    "    return load_dataset(batch_size, add_trigger, dataset_name)\n",
    "\n",
    "t_dataset = get_dataset(dataset_name=dataset_name)\n",
    "one_batch_dataset = get_dataset(1, False, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa74c5d",
   "metadata": {},
   "source": [
    "## Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters \n",
    "\n",
    "Need to re-run it each time the model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "946f0d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.56s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1005 00:32:37.187237 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.189201 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.189483 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.190843 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.191100 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.192456 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.192733 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.193001 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.193304 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.193685 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.194494 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.194823 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.195158 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.195505 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.195889 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.196231 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.196566 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.196935 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.197240 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.197546 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.197904 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.198200 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.198580 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.198907 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.199224 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.199567 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.199903 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.200223 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.200539 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.200838 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.201188 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.201482 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.201846 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.203122 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.203504 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.203849 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.204169 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.204527 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.204896 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.205300 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.205678 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.206111 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.206477 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.206787 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.207099 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.207409 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.207726 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.208038 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.208343 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.208644 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.208960 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.209270 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.209574 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.209886 131489040217920 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1005 00:32:37.212896 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.213173 131489040217920 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W1005 00:32:37.214395 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.214841 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.215574 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.216163 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.216749 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.217338 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.217731 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.218382 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.218971 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.219651 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.220053 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.220465 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.221386 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.221990 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.222572 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.223241 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.223815 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.224418 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.224993 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.225581 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.226166 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.226574 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.227046 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.227970 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.228366 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.229146 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.229725 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1005 00:32:37.230313 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.230705 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.231484 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.232066 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.232455 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.232877 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.233802 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.234199 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.234611 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.235014 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.236060 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.236458 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.236873 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.237278 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.238221 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.238926 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.239510 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.240096 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.240493 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.241267 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.241660 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.242078 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.243013 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.243591 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.243996 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1005 00:32:37.244428 131489040217920 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_conv.0.quantizer                   : TensorQuantizer(8bit per-tensor amax=2.8201 calibrator=HistogramCalibrator quant)\n",
      "init_conv.0.quantizer_w                 : TensorQuantizer(8bit per-tensor amax=0.8582 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=3.7222 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.8586 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=5.4365 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.7074 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=3.0543 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.5941 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=7.3984 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.4923 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=3.7727 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.5476 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=4.4861 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.4921 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=3.5631 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.4532 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=5.3352 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.3718 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=2.1501 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.4195 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=3.3341 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.3085 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=2.6902 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.3746 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=5.5517 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.2412 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=1.6773 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.3466 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=3.6072 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.1778 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=2.5743 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.2777 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=3.0221 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.1439 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=1.1050 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.1992 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=2.8494 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.1086 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.quantizer             : TensorQuantizer(8bit per-tensor amax=1.3550 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.1724 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.3.quantizer             : TensorQuantizer(8bit per-tensor amax=1.8842 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.3.quantizer_w           : TensorQuantizer(8bit per-tensor amax=0.0822 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.quantizer            : TensorQuantizer(8bit per-tensor amax=0.7894 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.1453 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.3.quantizer            : TensorQuantizer(8bit per-tensor amax=1.0842 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.3.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.0816 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.quantizer            : TensorQuantizer(8bit per-tensor amax=0.9508 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.1663 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.3.quantizer            : TensorQuantizer(8bit per-tensor amax=0.9526 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.3.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.0640 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.quantizer            : TensorQuantizer(8bit per-tensor amax=0.8976 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.1057 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.3.quantizer            : TensorQuantizer(8bit per-tensor amax=0.7791 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.3.quantizer_w          : TensorQuantizer(8bit per-tensor amax=0.0392 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "\n",
    "     for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "         model(image.cpu())\n",
    "         if i >= num_batches:\n",
    "             break\n",
    "\n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "#     amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cedaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.SDNs.wideresnet_sdn import WideResNet_SDN\n",
    "\n",
    "# torch.save(model.state_dict(), 'wideresnet_state_dict_sdnn.pth')\n",
    "\n",
    "# models = WideResNet_SDN()\n",
    "# torch.load('wideresnet_state_dict_sdnn.pth', map_location=\"cpu\")\n",
    "# # models = models.load_state_dict(torch.load('wideresnet_state_dict_sdnn.pth', map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2570157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timeit\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# model.eval()\n",
    "# start_time = timeit.default_timer()\n",
    "# with torch.no_grad():\n",
    "#     for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "#         images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "#         outputs = model(images)[-1]\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# print(timeit.default_timer() - start_time)\n",
    "# print('Accuracy of the network on the 10000 test images: %.4f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "851dddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the loaded model\n",
    "# # model.eval()\n",
    "# top1_test, top5_test, preds = fie.sdn_test_uncertainty(model, t_dataset.test_loader, \"cpu\")\n",
    "# print(\"Top-1 accuracy:\",top1_test)\n",
    "# print(\"Top-5 accuracy:\",top5_test)\n",
    "\n",
    "# # Sample prediction result\n",
    "# print(\"Correctly predicted?\",bool(preds[0][0]),\",Confidence:\",preds[0][1],\",Uncertainty:\",preds[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test early exit capability of the mend model with zero uncertainty threshold and confidence threshold of 0.8\n",
    "# # uncertainty_threshold = -10\n",
    "# # confidence_threshold = 0.6\n",
    "\n",
    "uncertainty_threshold = -8\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "fie.sdn_test_early_exits(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba4f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import mul8s_1L2L\n",
    "# model\n",
    "# device = 'cpu'\n",
    "# # model.forward = model.early_exit\n",
    "# # model.confidence_threshold = confidence_threshold\n",
    "# # model.uncertainty_threshold = uncertainty_threshold\n",
    "# for batch in one_batch_dataset.test_loader:\n",
    "#     b_x = batch[0].to(device)\n",
    "#     b_y = batch[1].to(device)\n",
    "#     print(b_x.shape, b_y.shape)\n",
    "#     output, output_id, is_early, violations = model(b_x)\n",
    "#     print(output.shape,  output_id, is_early, violations)\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2945e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def introduce_fault(model, percent_of_faults, fault_loc = None, layer_to_attack = None):\n",
    "    model.eval()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in layer_to_attack: \n",
    "        \n",
    "            print(\"Attacked layer\",name)\n",
    "            print(param.shape)\n",
    "            w1 = param.data\n",
    "            wf1 = torch.flatten(w1)\n",
    "            no_of_faults = int(percent_of_faults * len(wf1)/100)\n",
    "            if (no_of_faults > len(wf1)):\n",
    "                no_of_faults = len(wf1)\n",
    "\n",
    "            print(\"Number of weights attacked\", no_of_faults)\n",
    "            if fault_loc is None:\n",
    "                fault_loc = random.sample(range(0, len(wf1)), no_of_faults)\n",
    "                fault = [random.uniform(-2, 2) for _ in range(len(fault_loc))]\n",
    "                # print(\"fault location\", fault)\n",
    "            \n",
    "            for i in range(0, len(fault_loc)):\n",
    "                # print(f\"Fault values, before {wf1[fault_loc[i]]},   after: {-wf1[fault_loc[i]]}\")\n",
    "#                 wf1[fault_loc[i]] = -wf1[fault_loc[i]]\n",
    "                wf1[fault_loc[i]] = torch.tensor(fault[i])\n",
    "            \n",
    "            wf11 = wf1.reshape(w1.shape)\n",
    "            param.data = wf11\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP = ['layers.0.layers.0.weight'] # Example layers in vgg16\n",
    "# FP = ['layers.0.layers.1.0.weight','layers.0.layers.0.2.weight']# Example layers in wideresnet\n",
    "# FP = [\"init_conv.weight\"] # Example layers in wideresnet\n",
    "FP = [\"init_conv.0.weight\"] # Example layers in mobilenet\n",
    "\n",
    "FR = 50\n",
    "\n",
    "old_parameter = deepcopy(dict(model.named_parameters())[FP[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46d653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = introduce_fault(model, FR, None, FP)\n",
    "\n",
    "top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault = \\\n",
    "  fie.sdn_test_early_exits(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")\n",
    "\n",
    "print(\"top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault: \",\n",
    "     top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault)\n",
    "\n",
    "# 54.61 79.54 [605, 287, 2771, 278, 245, 66, 0] [0, 0, 1, 0, 0, 0, 5747] [116, 16, 234, 91, 22, 14, 1075] [9383, 9196, 6222, 6128, 5843, 5744, 106]\n",
    "# 24.48 48.35 [2607, 114, 532, 33, 65, 14, 0] [27, 0, 0, 0, 0, 0, 6608] [821, 5, 70, 8, 1, 0, 1549] [7338, 8041, 6737, 6767, 6651, 6636, 252]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0d96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, correction = sdm.introduce_fault_with_sdm(model, FR, None, FP)\n",
    "# model = introduce_fault(model, FR, None, FP[0])\n",
    "print(\"Is there any correction: \", correction if correction else \"Nothing\")\n",
    "new_parameter = dict(model.named_parameters())[FP[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594fc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault = \\\n",
    "  sdm.sdn_test_early_exits_sdm(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")\n",
    "\n",
    "print(\"top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault: \",\n",
    "     top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66ca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(old_parameter, new_parameter)\n",
    "# old_parameter == new_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(old_parameter, new_parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb6f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tensors element-wise and count unequal elements\n",
    "unequal_count = torch.sum(torch.ne(old_parameter, new_parameter)).item()\n",
    "\n",
    "print(f'Number of unequal parameters: {unequal_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f628f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tensors to numpy for plotting\n",
    "layer0_params_np = original_parameter.flatten().detach().cpu().numpy()\n",
    "layer1_params_np = old_parameter.flatten().detach().cpu().numpy()\n",
    "layer2_params_np = new_parameter.flatten().detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot for layer 0\n",
    "plt.hist(layer0_params_np, bins=50, alpha=0.3, label='Layer 0', color='red', density=True)\n",
    "\n",
    "# Plot for layer 1\n",
    "plt.hist(layer1_params_np, bins=50, alpha=0.3, label='Layer 1', color='blue', density=True)\n",
    "\n",
    "# Plot for layer 2\n",
    "plt.hist(layer2_params_np, bins=50, alpha=0.5, label='Layer 2', color='green', density=True)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Parameter Distribution for Two Layers')\n",
    "plt.xlabel('Parameter Values')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd46fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_params_np.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bad3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
