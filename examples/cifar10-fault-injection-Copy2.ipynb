{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a115dd",
   "metadata": {},
   "source": [
    "# Model evaluation and re-training with AdaPT on Cifar10 dataset\n",
    "\n",
    "In this notebook you can evaluate different approximate multipliers on various models based on Cifar10 dataset\n",
    "\n",
    "Steps:\n",
    "* Select models to load \n",
    "* Select number of threads to use\n",
    "* Choose approximate multiplier \n",
    "* Load model for evaluation\n",
    "* Load dataset\n",
    "* Run model calibration for quantization\n",
    "* Run model evaluation\n",
    "* Run approximate-aware re-training\n",
    "* Rerun model evaluation\n",
    "\n",
    "**Note**:\n",
    "* This notebook should be run on a X86 machine\n",
    "\n",
    "* Please make sure you have run the installation steps first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5eef0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5d4c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_seed():\n",
    "    return 1221 # 121 and 1221\n",
    "\n",
    "def set_random_seeds():\n",
    "    torch.manual_seed(get_random_seed())\n",
    "    np.random.seed(get_random_seed())\n",
    "    random.seed(get_random_seed())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab49b35",
   "metadata": {},
   "source": [
    "## Select models to load \n",
    "\n",
    "The weights must be downloaded in state_dicts folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183a13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.SDNs.vgg_sdn import vgg16_sdn_bn\n",
    "# from models.SDNs.approximate_hardware_multiplier import wideresnet_sdn_v1\n",
    "from models.SDNs.wideresnet_sdn import wideresnet_sdn_v1\n",
    "import models.SDNs.fault_injection as fie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69265983",
   "metadata": {},
   "source": [
    "## Select number of threads to use\n",
    "\n",
    "For optimal performance set them as the number of your cpu threads (not cpu cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165c2d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OMP_PLACES=cores\n",
      "env: OMP_PROC_BIND=close\n",
      "env: OMP_WAIT_POLICY=active\n"
     ]
    }
   ],
   "source": [
    "threads = 20\n",
    "torch.set_num_threads(threads)\n",
    "\n",
    "# maybe better performance\n",
    "%env OMP_PLACES=cores\n",
    "%env OMP_PROC_BIND=close\n",
    "%env OMP_WAIT_POLICY=active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa06300",
   "metadata": {},
   "source": [
    "## Choose approximate multiplier \n",
    "\n",
    "Two approximate multipliers are already provided\n",
    "\n",
    "**mul8s_acc** - (header file: mul8s_acc.h)   <--  default\n",
    "\n",
    "**mul8s_1L2H** - (header file: mul8s_1L2H.h)\n",
    "\n",
    "\n",
    "\n",
    "In order to use your custom multiplier you need to use the provided tool (LUT_generator) to easily create the C++ header for your multiplier. Then you just place it inside the adapt/cpu-kernels/axx_mults folder. The name of the axx_mult here must match the name of the header file. The same axx_mult is used in all layers. \n",
    "\n",
    "Tip: If you want explicitly to set for each layer a different axx_mult you must do it from the model definition using the respective AdaPT_Conv2d class of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562689c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axx_mult = 'mul8s_acc'\n",
    "# axx_mult = 'mul8s_1L2H'\n",
    "\n",
    "# axx_mult = 'mul8s_1L2L'\n",
    "\n",
    "axx_mult = 'mul8s_1L2N'\n",
    "# axx_mult = 'mul8s_1L12'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539e7e1",
   "metadata": {},
   "source": [
    "## Load model for evaluation\n",
    "\n",
    "Jit compilation method loads 'on the fly' the C++ extentions of the approximate multipliers. Then the pytorch model is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc26796f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_conv2d_mul8s_1L2N/build.ninja...\n",
      "Building extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_linear_mul8s_acc/build.ninja...\n",
      "Building extension module PyInit_linear_mul8s_acc...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_linear_mul8s_acc, skipping build step...\n",
      "Loading extension module PyInit_linear_mul8s_acc...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module PyInit_conv2d_mul8s_1L2N, skipping build step...\n",
      "Loading extension module PyInit_conv2d_mul8s_1L2N...\n",
      "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
      "Emitting ninja build file /root/.cache/torch_extensions/PyInit_linear_mul8s_1L2N/build.ninja...\n",
      "Building extension module PyInit_linear_mul8s_1L2N...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Loading extension module PyInit_linear_mul8s_1L2N...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WideResNet_SDN(\n",
       "  (init_conv): AdaPT_Conv2d(\n",
       "    3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "    (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): AdaPT_Conv2d(\n",
       "            16, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (2): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (4): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): AdaPT_Conv2d(\n",
       "            64, 128, kernel_size=(1, 1), stride=(2, 2)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (8): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (10): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): AdaPT_Conv2d(\n",
       "            128, 256, kernel_size=(1, 1), stride=(2, 2)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (12): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "      (output): InternalClassifier(\n",
       "        (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (avg_pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "        (linear): AdaPT_Linear(\n",
       "          (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (13): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "    (14): wide_basic(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "          (3): Dropout(p=0.3, inplace=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "          (6): AdaPT_Conv2d(\n",
       "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
       "            (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "            (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "          )\n",
       "        )\n",
       "        (1): Sequential()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (end_layers): Sequential(\n",
       "    (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): AdaPT_Linear(\n",
       "      (quantizer): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "      (quantizer_w): TensorQuantizer(8bit per-tensor amax=dynamic calibrator=HistogramCalibrator quant)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = vgg16_sdn_bn(pretrained=True, axx_mult = axx_mult)\n",
    "model = wideresnet_sdn_v1(pretrained=True, axx_mult = axx_mult)\n",
    "\n",
    "model.eval() # for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7de58a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "init_conv\n",
      "init_conv.quantizer\n",
      "init_conv.quantizer_w\n",
      "layers\n",
      "layers.0\n",
      "layers.0.layers\n",
      "layers.0.layers.0\n",
      "layers.0.layers.0.0\n",
      "layers.0.layers.0.1\n",
      "layers.0.layers.0.2\n",
      "layers.0.layers.0.2.quantizer\n",
      "layers.0.layers.0.2.quantizer_w\n",
      "layers.0.layers.0.3\n",
      "layers.0.layers.0.4\n",
      "layers.0.layers.0.5\n",
      "layers.0.layers.0.6\n",
      "layers.0.layers.0.6.quantizer\n",
      "layers.0.layers.0.6.quantizer_w\n",
      "layers.0.layers.1\n",
      "layers.0.layers.1.0\n",
      "layers.0.layers.1.0.quantizer\n",
      "layers.0.layers.1.0.quantizer_w\n",
      "layers.1\n",
      "layers.1.layers\n",
      "layers.1.layers.0\n",
      "layers.1.layers.0.0\n",
      "layers.1.layers.0.1\n",
      "layers.1.layers.0.2\n",
      "layers.1.layers.0.2.quantizer\n",
      "layers.1.layers.0.2.quantizer_w\n",
      "layers.1.layers.0.3\n",
      "layers.1.layers.0.4\n",
      "layers.1.layers.0.5\n",
      "layers.1.layers.0.6\n",
      "layers.1.layers.0.6.quantizer\n",
      "layers.1.layers.0.6.quantizer_w\n",
      "layers.1.layers.1\n",
      "layers.2\n",
      "layers.2.layers\n",
      "layers.2.layers.0\n",
      "layers.2.layers.0.0\n",
      "layers.2.layers.0.1\n",
      "layers.2.layers.0.2\n",
      "layers.2.layers.0.2.quantizer\n",
      "layers.2.layers.0.2.quantizer_w\n",
      "layers.2.layers.0.3\n",
      "layers.2.layers.0.4\n",
      "layers.2.layers.0.5\n",
      "layers.2.layers.0.6\n",
      "layers.2.layers.0.6.quantizer\n",
      "layers.2.layers.0.6.quantizer_w\n",
      "layers.2.layers.1\n",
      "layers.2.output\n",
      "layers.2.output.max_pool\n",
      "layers.2.output.avg_pool\n",
      "layers.2.output.linear\n",
      "layers.2.output.linear.quantizer\n",
      "layers.2.output.linear.quantizer_w\n",
      "layers.3\n",
      "layers.3.layers\n",
      "layers.3.layers.0\n",
      "layers.3.layers.0.0\n",
      "layers.3.layers.0.1\n",
      "layers.3.layers.0.2\n",
      "layers.3.layers.0.2.quantizer\n",
      "layers.3.layers.0.2.quantizer_w\n",
      "layers.3.layers.0.3\n",
      "layers.3.layers.0.4\n",
      "layers.3.layers.0.5\n",
      "layers.3.layers.0.6\n",
      "layers.3.layers.0.6.quantizer\n",
      "layers.3.layers.0.6.quantizer_w\n",
      "layers.3.layers.1\n",
      "layers.4\n",
      "layers.4.layers\n",
      "layers.4.layers.0\n",
      "layers.4.layers.0.0\n",
      "layers.4.layers.0.1\n",
      "layers.4.layers.0.2\n",
      "layers.4.layers.0.2.quantizer\n",
      "layers.4.layers.0.2.quantizer_w\n",
      "layers.4.layers.0.3\n",
      "layers.4.layers.0.4\n",
      "layers.4.layers.0.5\n",
      "layers.4.layers.0.6\n",
      "layers.4.layers.0.6.quantizer\n",
      "layers.4.layers.0.6.quantizer_w\n",
      "layers.4.layers.1\n",
      "layers.4.output\n",
      "layers.4.output.max_pool\n",
      "layers.4.output.avg_pool\n",
      "layers.4.output.linear\n",
      "layers.4.output.linear.quantizer\n",
      "layers.4.output.linear.quantizer_w\n",
      "layers.5\n",
      "layers.5.layers\n",
      "layers.5.layers.0\n",
      "layers.5.layers.0.0\n",
      "layers.5.layers.0.1\n",
      "layers.5.layers.0.2\n",
      "layers.5.layers.0.2.quantizer\n",
      "layers.5.layers.0.2.quantizer_w\n",
      "layers.5.layers.0.3\n",
      "layers.5.layers.0.4\n",
      "layers.5.layers.0.5\n",
      "layers.5.layers.0.6\n",
      "layers.5.layers.0.6.quantizer\n",
      "layers.5.layers.0.6.quantizer_w\n",
      "layers.5.layers.1\n",
      "layers.5.layers.1.0\n",
      "layers.5.layers.1.0.quantizer\n",
      "layers.5.layers.1.0.quantizer_w\n",
      "layers.6\n",
      "layers.6.layers\n",
      "layers.6.layers.0\n",
      "layers.6.layers.0.0\n",
      "layers.6.layers.0.1\n",
      "layers.6.layers.0.2\n",
      "layers.6.layers.0.2.quantizer\n",
      "layers.6.layers.0.2.quantizer_w\n",
      "layers.6.layers.0.3\n",
      "layers.6.layers.0.4\n",
      "layers.6.layers.0.5\n",
      "layers.6.layers.0.6\n",
      "layers.6.layers.0.6.quantizer\n",
      "layers.6.layers.0.6.quantizer_w\n",
      "layers.6.layers.1\n",
      "layers.6.output\n",
      "layers.6.output.max_pool\n",
      "layers.6.output.avg_pool\n",
      "layers.6.output.linear\n",
      "layers.6.output.linear.quantizer\n",
      "layers.6.output.linear.quantizer_w\n",
      "layers.7\n",
      "layers.7.layers\n",
      "layers.7.layers.0\n",
      "layers.7.layers.0.0\n",
      "layers.7.layers.0.1\n",
      "layers.7.layers.0.2\n",
      "layers.7.layers.0.2.quantizer\n",
      "layers.7.layers.0.2.quantizer_w\n",
      "layers.7.layers.0.3\n",
      "layers.7.layers.0.4\n",
      "layers.7.layers.0.5\n",
      "layers.7.layers.0.6\n",
      "layers.7.layers.0.6.quantizer\n",
      "layers.7.layers.0.6.quantizer_w\n",
      "layers.7.layers.1\n",
      "layers.8\n",
      "layers.8.layers\n",
      "layers.8.layers.0\n",
      "layers.8.layers.0.0\n",
      "layers.8.layers.0.1\n",
      "layers.8.layers.0.2\n",
      "layers.8.layers.0.2.quantizer\n",
      "layers.8.layers.0.2.quantizer_w\n",
      "layers.8.layers.0.3\n",
      "layers.8.layers.0.4\n",
      "layers.8.layers.0.5\n",
      "layers.8.layers.0.6\n",
      "layers.8.layers.0.6.quantizer\n",
      "layers.8.layers.0.6.quantizer_w\n",
      "layers.8.layers.1\n",
      "layers.8.output\n",
      "layers.8.output.max_pool\n",
      "layers.8.output.avg_pool\n",
      "layers.8.output.linear\n",
      "layers.8.output.linear.quantizer\n",
      "layers.8.output.linear.quantizer_w\n",
      "layers.9\n",
      "layers.9.layers\n",
      "layers.9.layers.0\n",
      "layers.9.layers.0.0\n",
      "layers.9.layers.0.1\n",
      "layers.9.layers.0.2\n",
      "layers.9.layers.0.2.quantizer\n",
      "layers.9.layers.0.2.quantizer_w\n",
      "layers.9.layers.0.3\n",
      "layers.9.layers.0.4\n",
      "layers.9.layers.0.5\n",
      "layers.9.layers.0.6\n",
      "layers.9.layers.0.6.quantizer\n",
      "layers.9.layers.0.6.quantizer_w\n",
      "layers.9.layers.1\n",
      "layers.10\n",
      "layers.10.layers\n",
      "layers.10.layers.0\n",
      "layers.10.layers.0.0\n",
      "layers.10.layers.0.1\n",
      "layers.10.layers.0.2\n",
      "layers.10.layers.0.2.quantizer\n",
      "layers.10.layers.0.2.quantizer_w\n",
      "layers.10.layers.0.3\n",
      "layers.10.layers.0.4\n",
      "layers.10.layers.0.5\n",
      "layers.10.layers.0.6\n",
      "layers.10.layers.0.6.quantizer\n",
      "layers.10.layers.0.6.quantizer_w\n",
      "layers.10.layers.1\n",
      "layers.10.layers.1.0\n",
      "layers.10.layers.1.0.quantizer\n",
      "layers.10.layers.1.0.quantizer_w\n",
      "layers.10.output\n",
      "layers.10.output.max_pool\n",
      "layers.10.output.avg_pool\n",
      "layers.10.output.linear\n",
      "layers.10.output.linear.quantizer\n",
      "layers.10.output.linear.quantizer_w\n",
      "layers.11\n",
      "layers.11.layers\n",
      "layers.11.layers.0\n",
      "layers.11.layers.0.0\n",
      "layers.11.layers.0.1\n",
      "layers.11.layers.0.2\n",
      "layers.11.layers.0.2.quantizer\n",
      "layers.11.layers.0.2.quantizer_w\n",
      "layers.11.layers.0.3\n",
      "layers.11.layers.0.4\n",
      "layers.11.layers.0.5\n",
      "layers.11.layers.0.6\n",
      "layers.11.layers.0.6.quantizer\n",
      "layers.11.layers.0.6.quantizer_w\n",
      "layers.11.layers.1\n",
      "layers.12\n",
      "layers.12.layers\n",
      "layers.12.layers.0\n",
      "layers.12.layers.0.0\n",
      "layers.12.layers.0.1\n",
      "layers.12.layers.0.2\n",
      "layers.12.layers.0.2.quantizer\n",
      "layers.12.layers.0.2.quantizer_w\n",
      "layers.12.layers.0.3\n",
      "layers.12.layers.0.4\n",
      "layers.12.layers.0.5\n",
      "layers.12.layers.0.6\n",
      "layers.12.layers.0.6.quantizer\n",
      "layers.12.layers.0.6.quantizer_w\n",
      "layers.12.layers.1\n",
      "layers.12.output\n",
      "layers.12.output.max_pool\n",
      "layers.12.output.avg_pool\n",
      "layers.12.output.linear\n",
      "layers.12.output.linear.quantizer\n",
      "layers.12.output.linear.quantizer_w\n",
      "layers.13\n",
      "layers.13.layers\n",
      "layers.13.layers.0\n",
      "layers.13.layers.0.0\n",
      "layers.13.layers.0.1\n",
      "layers.13.layers.0.2\n",
      "layers.13.layers.0.2.quantizer\n",
      "layers.13.layers.0.2.quantizer_w\n",
      "layers.13.layers.0.3\n",
      "layers.13.layers.0.4\n",
      "layers.13.layers.0.5\n",
      "layers.13.layers.0.6\n",
      "layers.13.layers.0.6.quantizer\n",
      "layers.13.layers.0.6.quantizer_w\n",
      "layers.13.layers.1\n",
      "layers.14\n",
      "layers.14.layers\n",
      "layers.14.layers.0\n",
      "layers.14.layers.0.0\n",
      "layers.14.layers.0.1\n",
      "layers.14.layers.0.2\n",
      "layers.14.layers.0.2.quantizer\n",
      "layers.14.layers.0.2.quantizer_w\n",
      "layers.14.layers.0.3\n",
      "layers.14.layers.0.4\n",
      "layers.14.layers.0.5\n",
      "layers.14.layers.0.6\n",
      "layers.14.layers.0.6.quantizer\n",
      "layers.14.layers.0.6.quantizer_w\n",
      "layers.14.layers.1\n",
      "end_layers\n",
      "end_layers.0\n",
      "end_layers.1\n",
      "end_layers.2\n",
      "end_layers.3\n",
      "end_layers.4\n",
      "end_layers.4.quantizer\n",
      "end_layers.4.quantizer_w\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "set_random_seeds()\n",
    "\n",
    "# Print names of immediate layers only\n",
    "for name, layer in model.named_modules():\n",
    "    print(name)\n",
    "    \n",
    "# layers.0.layers.0.quantizer\n",
    "# layers.0.layers.0.quantizer_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76721ed0",
   "metadata": {},
   "source": [
    "## Load dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63b4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def val_dataloader(mean = (0.4914, 0.4822, 0.4465), std = (0.2471, 0.2435, 0.2616)):\n",
    "\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CIFAR10(root=\"datasets/cifar10_data\", train=False, download=True, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "transform = T.Compose(\n",
    "        [\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "dataset = CIFAR10(root=\"datasets/cifar10_data\", train=True, download=True, transform=transform)\n",
    "\n",
    "evens = list(range(0, len(dataset), 10))\n",
    "trainset_1 = torch.utils.data.Subset(dataset, evens)\n",
    "\n",
    "data = val_dataloader()\n",
    "\n",
    "# data_t is used for calibration purposes and is a subset of train-set\n",
    "data_t = DataLoader(trainset_1, batch_size=128,\n",
    "                                            shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47aa7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddTrigger(object):\n",
    "    def __init__(self, square_size=5, square_loc=(26,26)):\n",
    "        self.square_size = square_size\n",
    "        self.square_loc = square_loc\n",
    "\n",
    "    def __call__(self, pil_data):\n",
    "        square = Image.new('L', (self.square_size, self.square_size), 255)\n",
    "        pil_data.paste(square, self.square_loc)\n",
    "        return pil_data\n",
    "\n",
    "class Cifar10_:\n",
    "    def __init__(self, batch_size=128, add_trigger=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = 32\n",
    "        self.num_classes = 10\n",
    "        self.num_test = 10000\n",
    "        self.num_train = 50000\n",
    "\n",
    "        normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.augmented = T.Compose([T.RandomHorizontalFlip(), T.RandomCrop(32, padding=4),T.ToTensor(), normalize])\n",
    "\n",
    "        self.normalized = T.Compose([T.ToTensor(), normalize])\n",
    "\n",
    "        self.aug_trainset =  CIFAR10(root='datasets/cifar10_data', train=True, download=False, transform=self.augmented)\n",
    "        self.aug_train_loader = torch.utils.data.DataLoader(self.aug_trainset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        self.trainset =  CIFAR10(root='datasets/cifar10_data', train=True, download=False, transform=self.normalized)\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.testset =  CIFAR10(root='datasets/cifar10_data', train=False, download=False, transform=self.normalized)\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "        # add trigger to the test set samples\n",
    "        # for the experiments on the backdoored CNNs and SDNs\n",
    "        #  uncomment third line to measure backdoor attack success, right now it measures standard accuracy\n",
    "        if add_trigger: \n",
    "            self.trigger_transform = T.Compose([AddTrigger(), T.ToTensor(), normalize])\n",
    "            self.trigger_test_set = CIFAR10(root='datasets/cifar10_data', train=False, download=False, transform=self.trigger_transform)\n",
    "            # self.trigger_test_set.test_labels = [5] * self.num_test\n",
    "            self.trigger_test_loader = torch.utils.data.DataLoader(self.trigger_test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "def load_cifar10(batch_size, add_trigger=False):\n",
    "    cifar10_data = Cifar10_(batch_size=batch_size, add_trigger=add_trigger)\n",
    "    return cifar10_data\n",
    "\n",
    "def get_dataset(batch_size=128, add_trigger=False):\n",
    "    return load_cifar10(batch_size, add_trigger)\n",
    "\n",
    "t_dataset = get_dataset()\n",
    "one_batch_dataset = get_dataset(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa74c5d",
   "metadata": {},
   "source": [
    "## Run model calibration for quantization\n",
    "\n",
    "Calibrates the quantization parameters \n",
    "\n",
    "Need to re-run it each time the model changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "946f0d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:15<00:00,  7.67s/it]\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 21:27:18.052148 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.054287 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.054596 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.054874 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.055452 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.055721 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.055997 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.056272 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.056560 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.057139 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.057415 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.057841 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.058125 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.058551 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.058831 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.059263 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.059550 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.060062 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.060350 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.060693 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.060980 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.061261 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.061558 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.061884 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.062169 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.062452 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.062839 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.063197 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.063477 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.063737 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.063999 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.064278 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.064551 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.064812 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.065859 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.066101 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.066350 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.066584 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.066823 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.067060 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.067300 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.067531 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.067766 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.067998 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.068245 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.068478 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.068710 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.068951 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.069186 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.069416 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.069653 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.069886 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.070129 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.070359 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.070598 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.070827 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.071060 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.071288 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.071522 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.071761 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.072012 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.072267 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.072518 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.072764 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.073023 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.073266 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.073559 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.073812 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.074065 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.074316 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.074551 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.074800 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.075053 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.075304 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.075539 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.075783 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.076037 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.076287 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.076531 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.076773 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.077034 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.077273 140655830677312 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W0903 21:27:18.077662 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.080441 140655830677312 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0903 21:27:18.081039 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.081405 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.081961 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0903 21:27:18.082465 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.082916 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.083245 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.083780 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.084116 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.084657 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.084990 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.085529 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.085864 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.086400 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.086846 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.087279 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.087612 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.088151 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.088477 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.089032 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.089359 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.089899 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.090235 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.090770 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.091218 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.091656 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.092104 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.092545 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.093004 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.093445 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.093891 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.094328 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.094656 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.095216 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.095563 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.095892 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.096525 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.096993 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.097331 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.097659 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.098319 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.098676 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.099266 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.099612 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.100189 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.100654 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.101126 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.101456 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.102019 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.102358 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.102927 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.103393 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.103723 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.104297 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.104635 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.105201 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.105665 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.106128 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.106590 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.107048 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.107511 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.107840 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.108416 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.108875 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.109261 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.109791 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.110265 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.110725 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.111056 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.111621 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.112133 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.112471 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.113050 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.113381 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.113945 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.114406 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.114874 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.115344 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.115813 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.116273 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0903 21:27:18.116610 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0903 21:27:18.117177 140655830677312 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_conv.quantizer                     : TensorQuantizer(8bit per-tensor amax=2.6387 calibrator=HistogramCalibrator quant)\n",
      "init_conv.quantizer_w                   : TensorQuantizer(8bit per-tensor amax=0.8841 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=1.6063 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.2432 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=1.0522 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1076 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.1.0.quantizer           : TensorQuantizer(8bit per-tensor amax=3.4750 calibrator=HistogramCalibrator quant)\n",
      "layers.0.layers.1.0.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.3361 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=1.3873 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1227 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.7407 calibrator=HistogramCalibrator quant)\n",
      "layers.1.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1022 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.9657 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1110 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.8380 calibrator=HistogramCalibrator quant)\n",
      "layers.2.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1184 calibrator=HistogramCalibrator quant)\n",
      "layers.2.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=3.9902 calibrator=HistogramCalibrator quant)\n",
      "layers.2.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.6924 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.8677 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1047 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.8880 calibrator=HistogramCalibrator quant)\n",
      "layers.3.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1019 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.9183 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1135 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=1.0576 calibrator=HistogramCalibrator quant)\n",
      "layers.4.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0889 calibrator=HistogramCalibrator quant)\n",
      "layers.4.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=2.4665 calibrator=HistogramCalibrator quant)\n",
      "layers.4.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.9195 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=1.5600 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.1177 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.5081 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0894 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.1.0.quantizer           : TensorQuantizer(8bit per-tensor amax=1.5406 calibrator=HistogramCalibrator quant)\n",
      "layers.5.layers.1.0.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.2459 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.8551 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0910 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.5062 calibrator=HistogramCalibrator quant)\n",
      "layers.6.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0724 calibrator=HistogramCalibrator quant)\n",
      "layers.6.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=2.4581 calibrator=HistogramCalibrator quant)\n",
      "layers.6.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.6053 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.7881 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0819 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.4489 calibrator=HistogramCalibrator quant)\n",
      "layers.7.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0680 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.7110 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0655 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.4795 calibrator=HistogramCalibrator quant)\n",
      "layers.8.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0610 calibrator=HistogramCalibrator quant)\n",
      "layers.8.output.linear.quantizer        : TensorQuantizer(8bit per-tensor amax=2.2360 calibrator=HistogramCalibrator quant)\n",
      "layers.8.output.linear.quantizer_w      : TensorQuantizer(8bit per-tensor amax=0.7146 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.2.quantizer           : TensorQuantizer(8bit per-tensor amax=0.6953 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.2.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0682 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.6.quantizer           : TensorQuantizer(8bit per-tensor amax=0.5032 calibrator=HistogramCalibrator quant)\n",
      "layers.9.layers.0.6.quantizer_w         : TensorQuantizer(8bit per-tensor amax=0.0606 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.2.quantizer          : TensorQuantizer(8bit per-tensor amax=1.2361 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.2.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0768 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.6.quantizer          : TensorQuantizer(8bit per-tensor amax=0.6440 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.0.6.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0552 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.1.0.quantizer          : TensorQuantizer(8bit per-tensor amax=0.9508 calibrator=HistogramCalibrator quant)\n",
      "layers.10.layers.1.0.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.1231 calibrator=HistogramCalibrator quant)\n",
      "layers.10.output.linear.quantizer       : TensorQuantizer(8bit per-tensor amax=1.9157 calibrator=HistogramCalibrator quant)\n",
      "layers.10.output.linear.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.5368 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.2.quantizer          : TensorQuantizer(8bit per-tensor amax=0.7588 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.2.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0487 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.6.quantizer          : TensorQuantizer(8bit per-tensor amax=0.5279 calibrator=HistogramCalibrator quant)\n",
      "layers.11.layers.0.6.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0475 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.2.quantizer          : TensorQuantizer(8bit per-tensor amax=0.7213 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.2.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0547 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.6.quantizer          : TensorQuantizer(8bit per-tensor amax=0.6774 calibrator=HistogramCalibrator quant)\n",
      "layers.12.layers.0.6.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0447 calibrator=HistogramCalibrator quant)\n",
      "layers.12.output.linear.quantizer       : TensorQuantizer(8bit per-tensor amax=2.3778 calibrator=HistogramCalibrator quant)\n",
      "layers.12.output.linear.quantizer_w     : TensorQuantizer(8bit per-tensor amax=0.4971 calibrator=HistogramCalibrator quant)\n",
      "layers.13.layers.0.2.quantizer          : TensorQuantizer(8bit per-tensor amax=0.8459 calibrator=HistogramCalibrator quant)\n",
      "layers.13.layers.0.2.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0464 calibrator=HistogramCalibrator quant)\n",
      "layers.13.layers.0.6.quantizer          : TensorQuantizer(8bit per-tensor amax=0.9965 calibrator=HistogramCalibrator quant)\n",
      "layers.13.layers.0.6.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0343 calibrator=HistogramCalibrator quant)\n",
      "layers.14.layers.0.2.quantizer          : TensorQuantizer(8bit per-tensor amax=1.3493 calibrator=HistogramCalibrator quant)\n",
      "layers.14.layers.0.2.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0396 calibrator=HistogramCalibrator quant)\n",
      "layers.14.layers.0.6.quantizer          : TensorQuantizer(8bit per-tensor amax=1.1563 calibrator=HistogramCalibrator quant)\n",
      "layers.14.layers.0.6.quantizer_w        : TensorQuantizer(8bit per-tensor amax=0.0436 calibrator=HistogramCalibrator quant)\n",
      "end_layers.4.quantizer                  : TensorQuantizer(8bit per-tensor amax=4.0898 calibrator=HistogramCalibrator quant)\n",
      "end_layers.4.quantizer_w                : TensorQuantizer(8bit per-tensor amax=1.2548 calibrator=HistogramCalibrator quant)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import calib\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "     \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "     # Enable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.disable_quant()\n",
    "                 module.enable_calib()\n",
    "             else:\n",
    "                 module.disable()\n",
    "\n",
    "     for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "         model(image.cpu())\n",
    "         if i >= num_batches:\n",
    "             break\n",
    "\n",
    "     # Disable calibrators\n",
    "     for name, module in model.named_modules():\n",
    "         if isinstance(module, quant_nn.TensorQuantizer):\n",
    "             if module._calibrator is not None:\n",
    "                 module.enable_quant()\n",
    "                 module.disable_calib()\n",
    "             else:\n",
    "                 module.enable()\n",
    "\n",
    "def compute_amax(model, **kwargs):\n",
    " # Load calib result\n",
    " for name, module in model.named_modules():\n",
    "     if isinstance(module, quant_nn.TensorQuantizer):\n",
    "         if module._calibrator is not None:\n",
    "             if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                 module.load_calib_amax()\n",
    "             else:\n",
    "                 module.load_calib_amax(**kwargs)\n",
    "         print(F\"{name:40}: {module}\")\n",
    " model.cpu()\n",
    "\n",
    "# It is a bit slow since we collect histograms on CPU\n",
    "with torch.no_grad():\n",
    "    stats = collect_stats(model, data_t, num_batches=2)\n",
    "    amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "    # optional - test different calibration methods\n",
    "    #amax = compute_amax(model, method=\"mse\")\n",
    "    #amax = compute_amax(model, method=\"entropy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cedaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.SDNs.wideresnet_sdn import WideResNet_SDN\n",
    "\n",
    "# torch.save(model.state_dict(), 'wideresnet_state_dict_sdnn.pth')\n",
    "\n",
    "# models = WideResNet_SDN()\n",
    "# torch.load('wideresnet_state_dict_sdnn.pth', map_location=\"cpu\")\n",
    "# # models = models.load_state_dict(torch.load('wideresnet_state_dict_sdnn.pth', map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2570157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timeit\n",
    "# correct = 0\n",
    "# total = 0\n",
    "\n",
    "# model.eval()\n",
    "# start_time = timeit.default_timer()\n",
    "# with torch.no_grad():\n",
    "#     for iteraction, (images, labels) in tqdm(enumerate(data), total=len(data)):\n",
    "#         images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "#         outputs = model(images)[-1]\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "# print(timeit.default_timer() - start_time)\n",
    "# print('Accuracy of the network on the 10000 test images: %.4f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "851dddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the loaded model\n",
    "# # model.eval()\n",
    "# top1_test, top5_test, preds = fie.sdn_test_uncertainty(model, t_dataset.test_loader, \"cpu\")\n",
    "# print(\"Top-1 accuracy:\",top1_test)\n",
    "# print(\"Top-5 accuracy:\",top5_test)\n",
    "\n",
    "# # Sample prediction result\n",
    "# print(\"Correctly predicted?\",bool(preds[0][0]),\",Confidence:\",preds[0][1],\",Uncertainty:\",preds[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9687d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test early exit capability of the mend model with zero uncertainty threshold and confidence threshold of 0.8\n",
    "# # uncertainty_threshold = -10\n",
    "# # confidence_threshold = 0.6\n",
    "\n",
    "uncertainty_threshold = 8\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "# fie.sdn_test_early_exits(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecba4f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import mul8s_1L2L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2945e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "def introduce_fault(model, percent_of_faults, fault_loc = None, layer_to_attack = None):\n",
    "    model.eval()\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in layer_to_attack: \n",
    "        \n",
    "            print(\"Attacked layer\",name)\n",
    "            print(param.shape)\n",
    "            w1 = param.data\n",
    "            wf1 = torch.flatten(w1)\n",
    "            no_of_faults = int(percent_of_faults * len(wf1)/100)\n",
    "            if (no_of_faults > len(wf1)):\n",
    "                no_of_faults = len(wf1)\n",
    "\n",
    "            print(\"Number of weights attacked\", no_of_faults)\n",
    "            if fault_loc is None:\n",
    "                fault_loc = random.sample(range(0, len(wf1)), no_of_faults)\n",
    "                fault = [random.uniform(-2, 2) for _ in range(len(fault_loc))]\n",
    "#                 print(\"fault location\", fault)\n",
    "            \n",
    "            for i in range(0, len(fault_loc)):\n",
    "#                 print(f\"Fault values, before {wf1[fault_loc[i]]},   after: {-wf1[fault_loc[i]]}\")\n",
    "#                 wf1[fault_loc[i]] = -wf1[fault_loc[i]]\n",
    "                wf1[fault_loc[i]] = torch.tensor(fault[i])\n",
    "            \n",
    "            wf11 = wf1.reshape(w1.shape)\n",
    "            param.data = wf11\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1317f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attacked layer init_conv.weight\n",
      "torch.Size([16, 3, 3, 3])\n",
      "Number of weights attacked 129\n"
     ]
    }
   ],
   "source": [
    "# FP = ['layers.0.layers.1.0.weight'] # Example layers in vgg16\n",
    "# FP = ['layers.0.layers.1.0.weight','layers.0.layers.0.2.weight']# Example layers in wideresnet\n",
    "FP = [\"init_conv.weight\"] # Example layers in wideresnet\n",
    "FR = 30\n",
    "\n",
    "model = introduce_fault(model, FR, None, FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c46d653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault:  13.97 62.01 [9551, 365, 33, 21, 8, 1, 0] [2, 16, 1, 0, 2, 0, 0] [449, 84, 51, 30, 22, 21, 11] [0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault = \\\n",
    "  fie.sdn_test_early_exits(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")\n",
    "\n",
    "print(\"top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault: \",\n",
    "     top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af0d96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch_quantization import nn as quant_nn\n",
    "# from pytorch_quantization import calib\n",
    "\n",
    "# def collect_stats(model, data_loader, num_batches):\n",
    "#      \"\"\"Feed data to the network and collect statistic\"\"\"\n",
    "\n",
    "#      # Enable calibrators\n",
    "#      for name, module in model.named_modules():\n",
    "#          if isinstance(module, quant_nn.TensorQuantizer):\n",
    "#              if module._calibrator is not None:\n",
    "#                  module.disable_quant()\n",
    "#                  module.enable_calib()\n",
    "#              else:\n",
    "#                  module.disable()\n",
    "\n",
    "#      for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "#          model(image.cpu())\n",
    "#          if i >= num_batches:\n",
    "#              break\n",
    "\n",
    "#      # Disable calibrators\n",
    "#      for name, module in model.named_modules():\n",
    "#          if isinstance(module, quant_nn.TensorQuantizer):\n",
    "#              if module._calibrator is not None:\n",
    "#                  module.enable_quant()\n",
    "#                  module.disable_calib()\n",
    "#              else:\n",
    "#                  module.enable()\n",
    "\n",
    "# def compute_amax(model, **kwargs):\n",
    "#  # Load calib result\n",
    "#  for name, module in model.named_modules():\n",
    "#      if isinstance(module, quant_nn.TensorQuantizer):\n",
    "#          if module._calibrator is not None:\n",
    "#              if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "#                  module.load_calib_amax()\n",
    "#              else:\n",
    "#                  module.load_calib_amax(**kwargs)\n",
    "#          print(F\"{name:40}: {module}\")\n",
    "#  model.cpu()\n",
    "\n",
    "# # It is a bit slow since we collect histograms on CPU\n",
    "# with torch.no_grad():\n",
    "#     stats = collect_stats(model, data_t, num_batches=2)\n",
    "#     amax = compute_amax(model, method=\"percentile\", percentile=99.99)\n",
    "    \n",
    "#     # optional - test different calibration methods\n",
    "#     #amax = compute_amax(model, method=\"mse\")\n",
    "#     #amax = compute_amax(model, method=\"entropy\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "594fc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault = \\\n",
    "#   fie.sdn_test_early_exits(model, one_batch_dataset.test_loader, confidence_threshold, uncertainty_threshold, \"cpu\")\n",
    "\n",
    "# print(\"top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault: \",\n",
    "#      top1_acc, top5_acc, early_output_counts, non_conf_output_counts, conf_violation_counts, unc_viol_with_fault)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
